{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial Data Processing Methodology\n",
    "\n",
    "Methodology employed for processing geospatial data using the GeoPandas library in Python. The primary objective was to refine and clip polygons from the Land Use Plan (LUP) dataset based on boundaries defined in the Limit Subset dataset, ensuring non-overlapping property polygons.\n",
    "\n",
    "2. Data Preparation\n",
    "2.1. Property Filtering and Sorting:\n",
    "\n",
    "The dataset was filtered to retain properties registered from the year 2000 onwards.\n",
    "Properties were sorted chronologically, first by their registration year (anho_capa) and subsequently by their registration date (fecha_res).\n",
    "2.2. Baseline Establishment:\n",
    "\n",
    "An initial set of properties from the year 2000 was used to establish a baseline in the final_properties GeoDataFrame.\n",
    "3. Data Processing\n",
    "3.1. Polygon Subtraction:\n",
    "\n",
    "For each subsequent year (2001-2022), the following steps were undertaken:\n",
    "The geometries of older properties were subtracted from the current property to ensure no overlaps.\n",
    "The modified current property was appended to the final_properties GeoDataFrame.\n",
    "3.2. Yearly Subsets Creation:\n",
    "\n",
    "The dataset was segmented into yearly subsets. Each subset was saved as a separate GeoPackage for granularity.\n",
    "3.3. Data Validation:\n",
    "\n",
    "Duplicate put_id values in the limit_subset dataset were identified and addressed.\n",
    "Rows with empty geometries were filtered out.\n",
    "A subset of the LUP dataset, termed lup_subset, was created based on unique put_id values from limit_subset.\n",
    "3.4. Geometry Validation and Repair:\n",
    "\n",
    "Invalid geometries in both the lup_subset and limit_subset datasets were identified.\n",
    "A buffer operation was employed to repair any detected invalid geometries.\n",
    "4. Clipping Process\n",
    "4.1. Polygon Clipping:\n",
    "\n",
    "For each geometry in the limit_subset dataset:\n",
    "Corresponding polygons from the lup_subset were identified.\n",
    "These polygons were then clipped based on the current limit_subset geometry's boundaries.\n",
    "The resulting clipped polygons were appended to the final_properties GeoDataFrame.\n",
    "5. Final Output\n",
    "5.1. Saving Processed Data:\n",
    "\n",
    "The final_properties GeoDataFrame, which houses the clipped polygons, was saved as a GeoPackage. This dataset is primed for subsequent analysis or visualizatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import clip\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon, MultiPolygon, GeometryCollection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the current working directory\n",
    "current_dir = os.path.abspath('')\n",
    "\n",
    "# Search for the 'constants.py' file starting from the current directory and moving up the hierarchy\n",
    "project_root = current_dir\n",
    "while not os.path.isfile(os.path.join(project_root, 'constants.py')):\n",
    "    project_root = os.path.dirname(project_root)\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import  DATA_PATH, LUP, LIMIT, LUP_SUBSET,LIMIT_SUBSET, LUP_PRELABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile using geopandas\n",
    "limit = gpd.read_file(LIMIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lup = gpd.read_file(LUP, layer = 'lup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lup['anho_capa'] = lup['anho_capa'].astype(int)\n",
    "filtered_lup = lup[(lup['anho_capa'] >= 2000) & (lup['anho_capa'] <= 2012)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lup =filtered_lup.to_crs(limit.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lup.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filtered_lup['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid geometries in lup and limit_subset\n",
    "invalid_lup = filtered_lup[~filtered_lup.geometry.is_valid]\n",
    "len(invalid_lup['put_id'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are invalid geometries, you might want to repair them\n",
    "# One common method is to use the buffer operation with a distance of 0\n",
    "if not invalid_lup.empty:\n",
    "    filtered_lup.geometry = filtered_lup.buffer(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid geometries in lup and limit_subset\n",
    "invalid_lup = filtered_lup[~filtered_lup.geometry.is_valid]\n",
    "len(invalid_lup['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a custom_limit_subset from lup_subset by uniting polygons based on their put_id\n",
    "# Step 1: Group by 'put_id'\n",
    "grouped = filtered_lup.groupby('put_id')\n",
    "\n",
    "# Step 2: Union polygons within each group\n",
    "unioned_polygons = grouped['geometry'].apply(lambda x: x.unary_union)\n",
    "\n",
    "# Step 3: Create a new GeoDataFrame\n",
    "custom_limit_subset = gpd.GeoDataFrame(unioned_polygons, columns=['geometry'])\n",
    "custom_limit_subset.reset_index(inplace=True)\n",
    "\n",
    "# Ensure the CRS is consistent\n",
    "custom_limit_subset.crs = filtered_lup.crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lup.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with unique 'put_id' and 'anho_capa'\n",
    "unique_anho_capa = filtered_lup[['put_id', 'anho_capa']].drop_duplicates()\n",
    "\n",
    "# Merge 'anho_capa' into 'custom_limit_subset'\n",
    "custom_limit_subset = custom_limit_subset.merge(unique_anho_capa, on='put_id', how='left')\n",
    "\n",
    "# Create a DataFrame with unique 'put_id' and 'fecha_res'\n",
    "unique_fecha_res = limit[['put_id', 'fecha_res']].drop_duplicates()\n",
    "\n",
    "# Merge 'fecha_res' into 'custom_limit_subset'\n",
    "custom_limit_subset = custom_limit_subset.merge(unique_fecha_res, on='put_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_limit_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#limit = limit[['id', 'put_id', 'anho_capa','fecha_res', 'geometry' ]]\n",
    "\n",
    "filtered_limit = custom_limit_subset[(custom_limit_subset['anho_capa'] >= 2000) & (custom_limit_subset['anho_capa'] <= 2012)]\n",
    "#filtered_limit['area'] = filtered_limit['geometry'].area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort properties by registration year\n",
    "properties = filtered_limit.sort_values(by='anho_capa')\n",
    "\n",
    "# Convert fecha_res to datetime format\n",
    "properties['fecha_res'] = pd.to_datetime(properties['fecha_res'], errors='coerce')\n",
    "\n",
    "# For rows with NaT (Not a Timestamp) in fecha_res, assign a default date based on their year\n",
    "properties.loc[properties['fecha_res'].isna(), 'fecha_res'] = pd.to_datetime(properties['anho_capa'].astype(str) + '-01-01')\n",
    "\n",
    "# Sort properties by fecha_res\n",
    "properties = properties.sort_values(by='fecha_res')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty GeoDataFrame to store the final processed properties\n",
    "final_properties = gpd.GeoDataFrame(columns=properties.columns)\n",
    "\n",
    "# Add properties from the year 2000 to final_properties as the baseline\n",
    "final_properties = pd.concat([final_properties, properties[properties['anho_capa'] == 2000]])\n",
    "\n",
    "for year in range(2001, 2023):  # Loop from 2001 to 2022\n",
    "    # Get properties of the current year\n",
    "    current_year_properties = properties[properties['anho_capa'] == year]\n",
    "    \n",
    "    # Iterate over each property of the current year\n",
    "    for idx, current_property in current_year_properties.iterrows():\n",
    "        # Subtract geometries of older properties from the current property\n",
    "        for _, older_property in final_properties.iterrows():\n",
    "            current_property['geometry'] = current_property['geometry'].difference(older_property['geometry'])\n",
    "        \n",
    "        # Append the \"cut\" current property to the final_properties GeoDataFrame\n",
    "        final_properties = pd.concat([final_properties, current_property.to_frame().T])\n",
    "final_properties.crs = properties.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visual Check in Qgis\n",
    "\n",
    "'''output_path = os.path.join(DATA_PATH,'processing')\n",
    "\n",
    "# Convert the 'fecha_res' column to a string format\n",
    "final_properties['fecha_res'] = final_properties['fecha_res'].astype(str)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    # Save the GeoDataFrame as a GeoPackage\n",
    "# Define the filename for the GeoPackage\n",
    "\n",
    "filename = os.path.join(output_path, \"custom-limit-clip.gpkg\")\n",
    "final_properties.to_file(filename, driver=\"GPKG\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid geometries\n",
    "invalid_geoms = final_properties[~final_properties.geometry.is_valid]\n",
    "len(invalid_geoms['put_id'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are invalid geometries, fix them\n",
    "if len(invalid_geoms) > 0:\n",
    "    final_properties.geometry = final_properties.geometry.buffer(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid geometries\n",
    "invalid_geoms = final_properties[~final_properties.geometry.is_valid]\n",
    "len(invalid_geoms['put_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining output slivers of properties causes issues when trying to clip LUP so buffers applied and LUP not captured acquired in QGIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_properties.geometry = final_properties.buffer(1, join_style= 2)\n",
    "final_properties.geometry = final_properties.buffer(-1, join_style= 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid geometries\n",
    "invalid_geoms = final_properties[~final_properties.geometry.is_valid]\n",
    "len(invalid_geoms['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_properties.geometry.is_empty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with empty geometries\n",
    "final_properties = final_properties[~final_properties.geometry.is_empty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid geometries\n",
    "invalid_geoms = final_properties[~final_properties.geometry.is_valid]\n",
    "len(invalid_geoms['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_properties.geometry.is_empty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_properties.geometry = final_properties.buffer(-2, join_style= 2)\n",
    "final_properties.geometry = final_properties.buffer(2, join_style= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visual Check in Qgis\n",
    "\n",
    "output_path = os.path.join(DATA_PATH,'processing')\n",
    "\n",
    "# Convert the 'fecha_res' column to a string format\n",
    "final_properties['fecha_res'] = final_properties['fecha_res'].astype(str)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    # Save the GeoDataFrame as a GeoPackage\n",
    "# Define the filename for the GeoPackage\n",
    "\n",
    "filename = os.path.join(output_path, \"custom-limit-clip_buffered.gpkg\")\n",
    "final_properties.to_file(filename, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the shapefile using geopandas\n",
    "'''limit_subset = gpd.read_file(LIMIT_SUBSET)\n",
    "lup_subset  = gpd.read_file(LUP_SUBSET)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_subset = final_properties\n",
    "# Lup from 2000-2012\n",
    "lup_subset  = filtered_lup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(limit_subset['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lup_subset['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''keep=False: This argument specifies how to mark duplicates:\n",
    "If keep='first' (default), it would mark all duplicates as True except for the first occurrence.\n",
    "If keep='last', it would mark all duplicates as True except for the last occurrence.\n",
    "If keep=False, it marks all duplicates as True.\n",
    "'''\n",
    "print(limit_subset[limit_subset.duplicated(subset='put_id', keep=False)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_subset.geometry.is_empty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with empty geometries\n",
    "limit_subset = limit_subset[~limit_subset.geometry.is_empty]\n",
    "\n",
    "# Reset the index if needed\n",
    "limit_subset.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(limit_subset['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lup_subset.geometry.is_empty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for invalid geometries in lup and limit_subset\n",
    "invalid_lup = lup_subset[~lup_subset.geometry.is_valid]\n",
    "invalid_limit = limit_subset[~limit_subset.geometry.is_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(invalid_limit['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(invalid_lup['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially, lup_subset contains all land use plans\n",
    "# Keep a copy of the original lup_subset before filtering\n",
    "original_lup_subset = lup_subset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the unique 'put_id' values from limit_subset\n",
    "put_ids_to_subset = limit_subset['put_id'].unique()\n",
    "\n",
    "\n",
    "# Filter lup_subset to only include land use plans with a corresponding property border\n",
    "lup_subset = lup_subset[lup_subset['put_id'].isin(put_ids_to_subset)]\n",
    "\n",
    "\n",
    "# Find the land use plans that were excluded in the filtering process\n",
    "excluded_lup = original_lup_subset[~original_lup_subset['put_id'].isin(put_ids_to_subset)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The LUP that don't have a matching property limit polygon need to be added in.\n",
    "len(excluded_lup['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(original_lup_subset['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lup_subset['put_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty GeoDataFrame to store the clipped lup polygons\n",
    "final_lup= gpd.GeoDataFrame(columns=lup_subset.columns, crs=lup_subset.crs)\n",
    "\n",
    "# Outer loop: Iterate through each geometry in limit_subset\n",
    "for _, row in limit_subset.iterrows():\n",
    "    current_limit_geom = row.geometry\n",
    "    current_put_id = row['put_id']\n",
    "        # Check if the geometry is a GeometryCollection\n",
    "        \n",
    "    # Check if the geometry is a GeometryCollection\n",
    "    if isinstance(current_limit_geom, GeometryCollection):\n",
    "        # Create a MultiPolygon from the GeometryCollection\n",
    "        # by filtering out non-polygon geometries and flattening the collection\n",
    "        polygons = [geom for geom in current_limit_geom.geoms if isinstance(geom, (Polygon, MultiPolygon))]\n",
    "        current_limit_geom = MultiPolygon(polygons)\n",
    "        \n",
    "    # Gather all the lup_subset polygons with the same put_id\n",
    "    current_lup_polygons = lup_subset[lup_subset['put_id'] == current_put_id]\n",
    "    \n",
    "    # Clip the gathered lup polygons using the current limit_subset geometry\n",
    "    clipped_lup = gpd.clip(current_lup_polygons, current_limit_geom)\n",
    "    \n",
    "    # Ensure the clipped_lup has the same CRS as final_properties before concatenating\n",
    "    clipped_lup = clipped_lup.to_crs(final_lup.crs)\n",
    "    \n",
    "    # Append the clipped lup polygons to the final_properties GeoDataFrame\n",
    "    final_lup = pd.concat([final_lup, clipped_lup])\n",
    "\n",
    "# Reset the index of the result GeoDataFrame\n",
    "final_lup.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge fecha_res from limit to lup based on put_id\n",
    "lup_filtered_fres = final_lup.merge(limit_subset[['put_id', 'fecha_res']], on='put_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visual inspection in qgis\n",
    "'''output_path = os.path.join(DATA_PATH,'processing' )\n",
    "\n",
    "#lup_filtered_fres['fecha_res'] = lup_filtered_fres['fecha_res'].astype(str)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    # Save the GeoDataFrame as a GeoPackage\n",
    "# Define the filename for the GeoPackage\n",
    "\n",
    "filename = os.path.join(output_path, \"custom_clipped_lup.gpkg\")\n",
    "lup_filtered_fres.to_file(filename, driver=\"GPKG\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(excluded_lup[~excluded_lup.geometry.is_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''output_path = os.path.join(DATA_PATH,'processing' )\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    # Save the GeoDataFrame as a GeoPackage\n",
    "# Define the filename for the GeoPackage\n",
    "\n",
    "filename = os.path.join(output_path, \"excluded_lup.gpkg\")\n",
    "excluded_lup.to_file(filename, driver=\"GPKG\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the excluded and final properties to see what was missed during the process in qgis. From this i decided that using the lup_subset and taking the difference was the best route as that gave me all the lup plans that were missed. applied a -45 and 45 buffer. Manually stil had to clean lups that overlapped, for most part they were just duplicates in some cases they were from different years. In the case of different years I selected the oldest lup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prelabel = gpd.read_file(LUP_PRELABEL)\n",
    "prelabel = lup_filtered_fres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify the unique values of 'categoria_ant' for each 'grupo'\n",
    "unique_values_mapping = prelabel.groupby('grupo')['categoria_ant'].unique().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given unique_values_mapping as a dictionary from your previous code:\n",
    "unique_values_mapping = {\n",
    "    'AREA_AUTORIZADA': np.array(['A-HABILITAR', 'SIN COBERTURA']),\n",
    "    'BOSQUES': np.array(['FRANJAS', 'RESERVA-FORESTAL', 'PROTECCION-CAUCES', 'PROTECCION',\n",
    "                         'BOSQUETES', 'REGENERACION', 'FORESTACION', 'A-REFORESTAR',\n",
    "                         'REMANENTE', 'REFORESTACION', 'A-REGENERAR', 'MANEJO-FORESTAL']),\n",
    "    'EN_CONFLICTO': np.array(['EN-CONFLICTO']),\n",
    "    'OTRAS_COBERTURAS': np.array(['NO_FORESTAL', 'PASTO', 'AREA AFECTADA POR M*']),\n",
    "    'OTRAS_TIERRAS_FORESTALES': np.array(['MATORRAL', 'PALMARES'])\n",
    "}\n",
    "\n",
    "# Create a mapping dictionary for 'categoria_ant' values\n",
    "categoria_ant_to_grupo = {}\n",
    "for grupo, categorias in unique_values_mapping.items():\n",
    "    for categoria in categorias.tolist():  # Convert numpy array to list before iterating\n",
    "        categoria_ant_to_grupo[categoria] = grupo\n",
    "\n",
    "# Adjust the mapping for 'EN_CONFLICTO'\n",
    "categoria_ant_to_grupo['EN-CONFLICTO'] = 'BOSQUES'\n",
    "\n",
    "# Fill in the empty rows of 'grupo'\n",
    "prelabel.loc[prelabel['grupo'].isna(), 'grupo'] = prelabel.loc[prelabel['grupo'].isna(), 'categoria_ant'].map(categoria_ant_to_grupo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of NaN values in the 'grupo' column\n",
    "number_of_nans = prelabel['grupo'].isna().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of NaN values in 'grupo' column: {number_of_nans}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the unique 'categoria_ant' values where 'grupo' is NaN\n",
    "missing_categories = prelabel[prelabel['grupo'].isna()]['categoria_ant'].unique()\n",
    "\n",
    "# Print the missing categories\n",
    "print(\"Missing categories that were not mapped:\")\n",
    "print(missing_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all NaN values in 'categoria_ant' correspond to NaN values in 'grupo'\n",
    "nan_correspondence_check = prelabel[prelabel['categoria_ant'].isna()]['grupo'].isna().all()\n",
    "\n",
    "# Print the result of the check\n",
    "print(f\"All NaN values in 'categoria_ant' correspond to NaN values in 'grupo': {nan_correspondence_check}\")\n",
    "\n",
    "# Assign 'B-INUNDABLE' and 'CAMINO' to 'OTRAS_COBERTURAS' if 'grupo' is NaN\n",
    "prelabel.loc[(prelabel['categoria_ant'].isin(['B-INUNDABLE', 'CAMINO'])) & (prelabel['grupo'].isna()), 'grupo'] = 'OTRAS_COBERTURAS'\n",
    "\n",
    "# Re-check the number of NaN values in the 'grupo' column after the operation\n",
    "number_of_nans_after = prelabel['grupo'].isna().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of NaN values in 'grupo' column after the operation: {number_of_nans_after}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the remaining NaN values in 'grupo' with 'OTRAS_COBERTURAS'\n",
    "prelabel['grupo'].fillna('OTRAS_COBERTURAS', inplace=True)\n",
    "\n",
    "# Re-check the number of NaN values in the 'grupo' column after the operation\n",
    "number_of_nans_final = prelabel['grupo'].isna().sum()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Number of NaN values in 'grupo' column after final operation: {number_of_nans_final}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prelabel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_df = prelabel[['anho_capa', 'put_id', 'fecha_res', 'grupo', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(DATA_PATH,'processing')\n",
    "\n",
    "selected_columns_df['fecha_res'] = selected_columns_df['fecha_res'].astype(str)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    # Save the GeoDataFrame as a GeoPackage\n",
    "# Define the filename for the GeoPackage\n",
    "\n",
    "filename = os.path.join(output_path, \"labeled_dataset.gpkg\")\n",
    "selected_columns_df.to_file(filename, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the spatial difference\n",
    "difference = limit.overlay(selected_columns_df, how='difference')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_subset = difference[(difference['anho_capa'] >= 2000) & (difference['anho_capa'] <= 2012)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_subset.geometry = difference_subset.buffer(-49, join_style= 2)\n",
    "difference_subset.geometry = difference_subset.buffer(49, join_style= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols =  difference_subset[['anho_capa', 'put_id', 'fecha_res', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols.geometry.is_empty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with empty geometries\n",
    "diff_cols = diff_cols[~diff_cols.geometry.is_empty]\n",
    "diff_cols.geometry.is_empty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_diff = diff_cols[~diff_cols.geometry.is_valid]\n",
    "len(invalid_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_cols['grupo'] = 'unclassified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_aligned = diff_cols.reindex(columns=selected_columns_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qgis_ready = pd.concat([selected_columns_df, diff_aligned], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(DATA_PATH,'processing')\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    # Save the GeoDataFrame as a GeoPackage\n",
    "# Define the filename for the GeoPackage\n",
    "\n",
    "filename = os.path.join(output_path, \"qgis_ready.gpkg\")\n",
    "qgis_ready.to_file(filename, driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qgis\n",
    "\n",
    "qgis_ready was exported to remove polygons that were still overlapping that could not be coded to decide which should be removed. Majority of them were just duplicates of the same polygons with only a few requiring a decision to be made on which polygons should remain. The polygons that required decisions were decided on using the same principle that the earliest registered land use plan claims the area and does not allow any later land use plan over the same area. \n",
    "\n",
    "From the cleaned file, a dissolved file was created merging all polygons that were touching (keep disjoint features seperate) to make individual polygons based on year. This is brought in, slivers cleaned with buffers, and then eliminating any overlaps. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
