{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import rasterio\n",
    "from rasterio import features\n",
    "from rasterio.features import rasterize, geometry_mask\n",
    "from rasterio.transform import from_origin\n",
    "import fiona\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.path.abspath('')\n",
    "\n",
    "# Search for the 'constants.py' file starting from the current directory and moving up the hierarchy\n",
    "project_root = current_dir\n",
    "while not os.path.isfile(os.path.join(project_root, 'constants.py')):\n",
    "    project_root = os.path.dirname(project_root)\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import CLEAN, DISSOLVED, STUDY_BOUNDARY_PATH, DATA_PATH, HANSEN_LOSSYEAR_FILEPATH, HANSEN_TEN_MASK, DISSOLVED_CLEAN_YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = gpd.read_file(CLEAN) # Import the final clean lup_dataset with empty properites included from qgis\n",
    "dissolved = gpd.read_file(DISSOLVED) # clean was dissolved by year but also keeping features distinct providing limit boundaries of each property by put_id\n",
    "study_boundary = gpd.read_file(STUDY_BOUNDARY_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up of dissolved dataset\n",
    "Removing slivers, readding a lost property, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolved = dissolved.drop(columns='fid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_crs = 'EPSG:4326'\n",
    "output_extent = (-62.64186038139295, -25.354320073574613, -57.14929123970096, -19.287457970745013)\n",
    "output_resolution = (0.00026949458523585647, -0.00026949458523585647)\n",
    "study_boundary = study_boundary.to_crs(output_crs)\n",
    "study_area_bounds = study_boundary.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a  Polygons based one for each year of 2000 - 2012, this will be used to select the correspoding 10-year pixels.\n",
    "clean_dissolve = dissolved #clean.dissolve('anho_capa', as_index=False)\n",
    "# Clean Slivers\n",
    "clean_dissolve.geometry = clean_dissolve.buffer(25, join_style= 2)\n",
    "clean_dissolve.geometry = clean_dissolve.buffer(-25, join_style= 2)\n",
    "clean_dissolve = clean_dissolve.reindex(columns=dissolved.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty GeoDataFrame to store the final processed properties\n",
    "clean_years = gpd.GeoDataFrame(columns=dissolved.columns)\n",
    "\n",
    "# Add polygons from the year 2000 to clean_years as the baseline\n",
    "clean_years = pd.concat([clean_years, clean_dissolve[clean_dissolve['anho_capa'] == '2000']])\n",
    "for year in range(2001, 2013):  # Loop from 2001 to 2012\n",
    "    # Get polygons of the current year\n",
    "    current_year_properties = clean_dissolve[clean_dissolve['anho_capa'] == str(year)]\n",
    "    \n",
    "    # Iterate over each polygons of the current year\n",
    "    for idx, current_property in current_year_properties.iterrows():\n",
    "        # Subtract geometries of older polygons from the current polygons\n",
    "        for _, older_property in clean_years.iterrows():\n",
    "            current_property['geometry'] = current_property['geometry'].difference(older_property['geometry'])\n",
    "        \n",
    "        # Append the \"cut\" current polygons to the clean_years GeoDataFrame\n",
    "        clean_years = pd.concat([clean_years, current_property.to_frame().T])\n",
    "clean_years.crs = dissolved.crs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visual Check in Qgis\n",
    "\n",
    "\n",
    "'''output_path = os.path.join(DATA_PATH,'processing')\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    # Save the GeoDataFrame as a GeoPackage\n",
    "# Define the filename for the GeoPackage\n",
    "\n",
    "filename = os.path.join(output_path, \"clean_years.gpkg\")\n",
    "clean_years.to_file(filename, driver=\"GPKG\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_row = clean.loc[clean['put_id'] == 'PUT2238']\n",
    "\n",
    "selected_row_dissolved = selected_row.dissolve(by = 'anho_capa', as_index = False)\n",
    "selected_row_dissolved.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_row_dissolved.geometry = selected_row_dissolved.buffer(25, join_style= 2)\n",
    "selected_row_dissolved.geometry = selected_row_dissolved.buffer(-25, join_style= 2)\n",
    "selected_row_dissolved = selected_row_dissolved.reindex(columns=dissolved.columns)\n",
    "selected_row_dissolved = selected_row_dissolved.drop(columns=['put_id', 'fecha_res', 'grupo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Filter out non-polygon geometries\n",
    "clean_years_polygons = clean_years[clean_years['geometry'].apply(lambda geom: isinstance(geom, Polygon) or isinstance(geom, MultiPolygon))]\n",
    "\n",
    "# Step 2: Dissolve the filtered GeoDataFrame\n",
    "dissolved_clean_years = clean_years_polygons.dissolve(by='anho_capa', as_index=False)\n",
    "\n",
    "dissolved_clean_years = dissolved_clean_years.reindex(columns=dissolved.columns)\n",
    "dissolved_clean_years = dissolved_clean_years.drop(columns=['put_id', 'fecha_res', 'grupo'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolved_clean_years = pd.concat([dissolved_clean_years, selected_row_dissolved], ignore_index=True)\n",
    "# This is the final property limit dataset from here were create the spatial blocking dataset then to raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Visual Check in Qgis\n",
    "\n",
    "\n",
    "'''output_path = os.path.join(DATA_PATH,'processing')\n",
    "\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "    # Save the GeoDataFrame as a GeoPackage\n",
    "# Define the filename for the GeoPackage\n",
    "\n",
    "filename = os.path.join(output_path, \"dissolved_clean_years.gpkg\")\n",
    "dissolved_clean_years.to_file(filename, driver=\"GPKG\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolved_clean_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in point\n",
    "The above  will reproduce the dissolved_clean_years but to save time will read in the shapefile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolved_clean_years =  gpd.read_file(DISSOLVED_CLEAN_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolved_clean_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissolved_clean_years = dissolved_clean_years.to_crs('EPSG:4326')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio.mask\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where files will save, can add subfolders if desired\n",
    "output_dir = os.path.join(DATA_PATH, 'processed_rasters')\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure CRS match between shapefile and raster\n",
    "shapes = dissolved_clean_years.to_crs(crs=rasterio.open(HANSEN_TEN_MASK).crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raster\n",
    "with rasterio.open(HANSEN_TEN_MASK) as src:\n",
    "    # Crop the raster with the shapefile\n",
    "    out_image, out_transform = mask(src, shapes.geometry, crop=False)\n",
    "    out_meta = src.meta.copy()\n",
    "\n",
    "    # Update the metadata to reflect the new shape (height, width), transform, and nodata value\n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform,\n",
    "                     \"nodata\": -1})\n",
    "\n",
    "    # Save the cropped raster\n",
    "    output_raster_path = os.path.join(output_dir, \"cropped_hansen_ten_mask.tif\")\n",
    "    with rasterio.open(output_raster_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''cropped_raster_path = os.path.join(output_dir, \"cropped_hansen_ten_mask.tif\")\n",
    "with rasterio.open(cropped_raster_path) as src:\n",
    "    cropped_raster = src.read(1)\n",
    "    cropped_meta = src.meta\n",
    "\n",
    "    # Create an initial mask with all values set to -1\n",
    "    modified_raster = np.full(cropped_raster.shape, -1, dtype=cropped_raster.dtype)\n",
    "\n",
    "    # Process each polygon\n",
    "    for index, row in dissolved_clean_years.iterrows():\n",
    "        year = int(row['anho_capa'])  # Convert year to integer\n",
    "        polygon = row['geometry']\n",
    "\n",
    "        # Create a mask for the polygon\n",
    "        polygon_mask = geometry_mask([polygon], transform=src.transform, invert=True, out_shape=src.shape)\n",
    "\n",
    "        # Determine the range of years to keep\n",
    "        start_year = year - 2000 + 1  # Adjust based on your data encoding\n",
    "        end_year = start_year + 9\n",
    "\n",
    "        # Apply the mask and modify values within the polygon\n",
    "        within_polygon = (polygon_mask & (cropped_raster >= start_year) & (cropped_raster <= end_year))\n",
    "        modified_raster[within_polygon] = cropped_raster[within_polygon]\n",
    "\n",
    "    # Update the metadata for output\n",
    "    cropped_meta.update(dtype='int16', nodata=-1)\n",
    "\n",
    "    # Save the final modified raster\n",
    "    with rasterio.open('final_modified_raster.tif', 'w', **cropped_meta) as dst:\n",
    "        dst.write(modified_raster, 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cropped_raster_path = os.path.join(output_dir, \"cropped_hansen_ten_mask.tif\")\n",
    "with rasterio.open(cropped_raster_path) as src:\n",
    "    cropped_raster = src.read(1)\n",
    "    cropped_meta = src.meta\n",
    "\n",
    "    # Create an initial mask with all values set to -1\n",
    "    modified_raster = np.full(cropped_raster.shape, -1, dtype=cropped_raster.dtype)\n",
    "\n",
    "    # Process each polygon\n",
    "    for index, row in dissolved_clean_years.iterrows():\n",
    "        year = int(row['anho_capa'])  # Convert year to integer\n",
    "        polygon = row['geometry']\n",
    "\n",
    "        # Create a mask for the polygon\n",
    "        polygon_mask = geometry_mask([polygon], transform=src.transform, invert=True, out_shape=src.shape)\n",
    "\n",
    "        # Determine the range of years to keep\n",
    "        start_year = year - 2000 + 1  # Adjust based on your data encoding\n",
    "        end_year = start_year + 9\n",
    "\n",
    "        # Apply the mask and modify values within the polygon\n",
    "        within_polygon = polygon_mask\n",
    "        in_year_range = (cropped_raster >= start_year) & (cropped_raster <= end_year)\n",
    "\n",
    "        # Set values within the year range to their corresponding values\n",
    "        modified_raster[within_polygon & in_year_range] = cropped_raster[within_polygon & in_year_range]\n",
    "\n",
    "        # Set values outside the year range but within the polygon to 0\n",
    "        modified_raster[within_polygon & ~in_year_range] = 0\n",
    "\n",
    "    # Update the metadata for output\n",
    "    cropped_meta.update(dtype='int16', nodata=-1)\n",
    "\n",
    "    # Save the final modified raster\n",
    "    with rasterio.open('final_modified_raster0.tif', 'w', **cropped_meta) as dst:\n",
    "        dst.write(modified_raster, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector to Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_crs = 'EPSG:4326'\n",
    "output_extent = ( -62.6418603813929522, -25.3543200735746126, -57.1492912397009576, -19.2874579707450131)\n",
    "output_resolution = (0.000269494585235856472, -0.000269494585235856472)\n",
    "study_boundary = study_boundary.to_crs(output_crs)\n",
    "#study_area_bounds = study_boundary.total_bounds\n",
    "study_area_bounds = output_extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean.to_crs(output_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-62.64186038139295, -25.354320073574613, -57.14929123970096, -19.287457970745013)\n"
     ]
    }
   ],
   "source": [
    "print(output_extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-62.64186038139295,\n",
       " -25.354320073574613,\n",
       " -57.14929123970096,\n",
       " -19.287457970745013)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study_area_bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_to_raster(input_vector, output_raster, attribute, study_area_bounds, value_mapping, single_value=None, resolution=(abs(0.000269494585235856472), abs(-0.000269494585235856472)), dtype='int16'):\n",
    "    # Check if input_vector is a GeoDataFrame or a file path\n",
    "    if isinstance(input_vector, gpd.GeoDataFrame):\n",
    "        gdf = input_vector\n",
    "    else:\n",
    "        # Read the input vector file (GeoPackage or Shapefile) into a GeoDataFrame\n",
    "        gdf = gpd.read_file(input_vector)\n",
    "    # Reproject the GeoDataFrame to the desired CRS (EPSG:4326)\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "\n",
    "    # Ensure that categorical column is string\n",
    "    gdf[attribute] = gdf[attribute].astype(str)\n",
    "   \n",
    "\n",
    "   # If single_value is None, convert the attribute column to numerical values\n",
    "    # If single_value is provided, set the attribute column to the provided single_value\n",
    "    if single_value is None:\n",
    "        gdf[attribute] = gdf[attribute].replace(value_mapping).astype(dtype)\n",
    "    else:\n",
    "        gdf[attribute] = single_value\n",
    "\n",
    "\n",
    "    # Use the study area bounds to define the dimensions and transform of the output raster\n",
    "    minx, miny, maxx, maxy = study_area_bounds\n",
    "    width = int(np.ceil((maxx - minx) / abs(resolution[0])))\n",
    "    height = int(np.ceil((maxy - miny) / abs(resolution[1])))\n",
    "    out_transform = rasterio.transform.from_bounds(minx, miny, maxx, maxy, width, height)\n",
    "\n",
    "\n",
    "    # Define the metadata for the output raster file\n",
    "    out_meta = {\n",
    "        'driver': 'GTiff',\n",
    "        'width': 20381,\n",
    "        'height': 22512,\n",
    "        'count': 1,\n",
    "        'dtype': dtype,\n",
    "        'crs': 'EPSG:4326',\n",
    "        'transform': out_transform,\n",
    "        'nodata': -1  # Set the nodata value\n",
    "    }\n",
    "    \n",
    "    # Open the output raster file for writing with the specified metadata\n",
    "    with rasterio.open(output_raster, 'w', **out_meta) as dst:\n",
    "        # Create a generator of tuples containing the geometry and attribute value for each feature in the input vector data\n",
    "        shapes = ((geom, value) for geom, value in zip(gdf['geometry'], gdf[attribute]))\n",
    "        \n",
    "        # Burn the geometries and their corresponding attribute values into a raster array\n",
    "        burned = features.rasterize(\n",
    "            shapes=shapes,         # The generator of geometry-attribute tuples\n",
    "            fill=-1,                # The default value for pixels not covered by any geometry\n",
    "            out_shape=(22512, 20381), # The shape of the output raster array (number of rows and columns)\n",
    "            transform=out_transform,   # The affine transformation matrix that maps pixel coordinates to the coordinate reference system\n",
    "            dtype=dtype,            # The data type of the raster array\n",
    "            default_value=-1    # Set the nodata value\n",
    "        )\n",
    "        \n",
    "        # Write the burned raster array to the output raster file\n",
    "        dst.write(burned, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write text file of value mapping\n",
    "def write_value_mapping(value_mapping, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for key, value in value_mapping.items():\n",
    "            f.write(f'{key}: {value}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_columns(input_vector, output_dir, study_area_bounds, resolution, columns, value_mapping, single_value=None, file_name=None):\n",
    "    # Check if input_vector is a GeoDataFrame or a file path\n",
    "    if isinstance(input_vector, gpd.GeoDataFrame):\n",
    "        gdf = input_vector\n",
    "        if file_name is None:\n",
    "            file_name = \"output\"  # Set a default file name if none is provided\n",
    "    else:\n",
    "        # Read the input vector file (GeoPackage or Shapefile) into a GeoDataFrame\n",
    "        gdf = gpd.read_file(input_vector)\n",
    "        if file_name is None:\n",
    "            file_name = os.path.splitext(os.path.basename(input_vector))[0]\n",
    "\n",
    "    for column in columns:\n",
    "        column_output_dir = os.path.join(output_dir, column)\n",
    "        os.makedirs(column_output_dir, exist_ok=True)\n",
    "\n",
    "        output_raster = f\"{column_output_dir}/{file_name}_{column}_raster.tif\"\n",
    "        vector_to_raster(gdf, output_raster, column, study_area_bounds, value_mapping[column], single_value=single_value, resolution=resolution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Land Use Plan into Raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(input_data, output_dir, study_area_bounds, resolution, columns, single_value=None):\n",
    "    # Generate a global value mapping from the list of unique values for each column\n",
    "    global_value_mapping = {column: set() for column in columns}\n",
    "    \n",
    "    if isinstance(input_data, str):\n",
    "        gdf = gpd.read_file(input_data)\n",
    "    elif isinstance(input_data, gpd.GeoDataFrame):\n",
    "        gdf = input_data\n",
    "    else:\n",
    "        raise ValueError(\"Invalid input data type. Expected file path or GeoDataFrame.\")\n",
    "    \n",
    "    for column in columns:\n",
    "        global_value_mapping[column].update(gdf[column].astype(str).unique())\n",
    "    \n",
    "    for column in columns:\n",
    "        global_value_mapping[column] = {value: idx for idx, value in enumerate(sorted(list(global_value_mapping[column])), 1)}\n",
    "        print(f\"Global value mapping for {column}:\")\n",
    "        for value in global_value_mapping[column]:\n",
    "            print(f\"{value}: {global_value_mapping[column][value]}\")\n",
    "    \n",
    "    # Write the global value mapping for each column to separate .txt files in the corresponding column folder\n",
    "    for column in columns:\n",
    "        column_output_dir = os.path.join(output_dir, column)\n",
    "        os.makedirs(column_output_dir, exist_ok=True)\n",
    "\n",
    "        output_value_mapping_file = f\"{column_output_dir}/{column}_global_value_mapping.txt\"\n",
    "        write_value_mapping(global_value_mapping[column], output_value_mapping_file)\n",
    "\n",
    "    # Process the input data with the global value mapping\n",
    "    process_columns(gdf, output_dir, study_area_bounds, columns=columns, resolution=resolution, value_mapping=global_value_mapping, single_value=single_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global value mapping for grupo:\n",
      "AREA_AUTORIZADA: 1\n",
      "BOSQUES: 2\n",
      "OTRAS_COBERTURAS: 3\n",
      "OTRAS_TIERRAS_FORESTALES: 4\n",
      "unclassified: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bsf31\\AppData\\Local\\Temp\\ipykernel_21600\\3759962778.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  gdf[attribute] = gdf[attribute].replace(value_mapping).astype(dtype)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Call the process_files function for the clean GeoDataFrame\n",
    "process_files(clean, output_dir, study_area_bounds, output_resolution, columns = ['grupo'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the process_files function for the dissolved_clean_years GeoDataFrame\n",
    "process_files(dissolved_clean_years, output_dir, study_area_bounds, output_resolution, columns = ['anho_capa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_files(clean, output_dir, study_area_bounds, output_resolution, columns = ['put_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
