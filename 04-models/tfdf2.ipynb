{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kztx6LuVVWBX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gc  # Garbage collector interface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWkvr-Goe7vy"
      },
      "outputs": [],
      "source": [
        "\n",
        "import rasterio\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1DoTLm6WZdm"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score,\n",
        "                             precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                             precision_recall_curve, roc_curve, auc)\n",
        "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
        "                                     GridSearchCV, RandomizedSearchCV)\n",
        "\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "import joblib\n",
        "from joblib import dump\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from scipy.stats import skew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owrViUXGZ7Hx"
      },
      "outputs": [],
      "source": [
        "FEATURES_DIR = r\"C:\\Users\\bsf31\\Documents\\post-meds\\data\\policy-data\\ml_data\\training\"\n",
        "EXCLUDE_FILE = 'train_binary_deforestation_raster.tif'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFaLv6mJKtrJ"
      },
      "outputs": [],
      "source": [
        "# Helper function to read TIFF files\n",
        "def read_tiff_image(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        return src.read(1)\n",
        "\n",
        "# List of paths to the raster files excluding the specified file\n",
        "feature_files = [os.path.join(FEATURES_DIR, file_name)\n",
        "                 for file_name in os.listdir(FEATURES_DIR)\n",
        "                 if file_name != EXCLUDE_FILE]\n",
        "\n",
        "# Read and store each raster file's data in an array\n",
        "feature_data_arrays = [read_tiff_image(file_path) for file_path in feature_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXMBLpJc1CE"
      },
      "outputs": [],
      "source": [
        "feature_data_flat = [data_array.flatten() for data_array in feature_data_arrays]\n",
        "del feature_data_arrays\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dmiUDScc725",
        "outputId": "053f7622-66a3-4256-d07a-7c4f4ec34b5d"
      },
      "outputs": [],
      "source": [
        "feature_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xoktmVndVYJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Path to the y_file\n",
        "y_file = os.path.join(FEATURES_DIR, 'train_binary_deforestation_raster.tif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx1S_CpIcV4A"
      },
      "outputs": [],
      "source": [
        "\n",
        "# NoData Value\n",
        "no_data_value = -1\n",
        "\n",
        "# Stack the flattened raster data\n",
        "X_flat = np.column_stack(feature_data_flat)\n",
        "\n",
        "# Delete the raw feature arrays as they are no longer needed after flattening and cleaning\n",
        "del feature_data_flat\n",
        "\n",
        "# Use the y_file obtained from the find_deforestation_file function\n",
        "y = read_tiff_image(y_file).flatten()\n",
        "del y_file\n",
        "\n",
        "# Remove rows with NoData values\n",
        "'''checks each row in X_flat and creates a boolean array (valid_rows_X) that has the same number of elements\n",
        "as the number of rows in X_flat. Each element in valid_rows_X is True if there is no NoData value in\n",
        "the corresponding row of X_flat and False otherwise.'''\n",
        "valid_rows_X = ~(X_flat == no_data_value).any(axis=1)\n",
        "\n",
        "'''checks each element in the y array and creates a boolean array (valid_rows_y) that has the same number of\n",
        "elements as y. Each element in valid_rows_y is True if the corresponding element in y is not\n",
        "equal to the NoData value and False otherwise.'''\n",
        "valid_rows_y = y != no_data_value\n",
        "\n",
        "'''checks each element in the y array and creates a boolean array (valid_rows_y)\n",
        "that has the same number of elements as y. Each element in valid_rows_y is True if the corresponding element\n",
        "in y is not equal to the NoData value and False otherwise.'''\n",
        "valid_rows = valid_rows_X & valid_rows_y\n",
        "del valid_rows_X\n",
        "del valid_rows_y\n",
        "\n",
        "'''creates a new array X_cleaned by selecting only the rows in X_flat that\n",
        "correspond to the True elements in valid_rows.'''\n",
        "X_cleaned = X_flat[valid_rows]\n",
        "\n",
        "'''creates a new array y_cleaned by selecting only the elements in y that correspond\n",
        "to the True elements in valid_rows.'''\n",
        "y_cleaned = y[valid_rows]\n",
        "\n",
        "del X_flat\n",
        "del y\n",
        "del valid_rows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VU4G5p0PK0Y_"
      },
      "outputs": [],
      "source": [
        "# Define the labels for your features\n",
        "feature_labels = [ 'CITIES', 'GRUPO', 'PORTS', 'PRECIPITATION', 'RIVER', 'ROAD', 'SOIL' ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_cleaned[:, feature_labels.index('RIVER')] = np.log1p(X_cleaned[:, feature_labels.index('RIVER')])\n",
        "X_cleaned[:, feature_labels.index('CITIES')] = np.log1p(X_cleaned[:, feature_labels.index('CITIES')])\n",
        "X_cleaned[:, feature_labels.index('ROAD')] = np.log1p(X_cleaned[:, feature_labels.index('ROAD')])\n",
        "\n",
        "del feature_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWq1MnO6cweP"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.9, random_state=42, stratify=y_cleaned)\n",
        "del X_cleaned\n",
        "del y_cleaned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4fI7W_gsV1H"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype(np.float32)\n",
        "y_train = y_train.astype(np.int32)  # Convert y_train to int32\n",
        "X_test = X_test.astype(np.float32)\n",
        "y_test = y_test.astype(np.int32)  # Convert y_test to int32\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzkRlvgWeug2",
        "outputId": "0bfb37a0-2c0d-49a1-9d87-1ea0c8c7179b"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Force a garbage collection to free up unused memory\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "cKIubf20c2XS",
        "outputId": "ff1a3091-44f5-4338-d72a-c3293b73daa2"
      },
      "outputs": [],
      "source": [
        "brfc = BalancedRandomForestClassifier(random_state=42, class_weight= 'balanced', sampling_strategy='not majority')\n",
        "\n",
        "# Define a basic parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],   # number of trees in the forest\n",
        "    'max_depth': [None, 5, 10, 20],    # maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10],   # minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 4],     # minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['sqrt']   # number of features to consider when looking for the best split\n",
        "}\n",
        "\n",
        "# Set scoring metrics\n",
        "scoring = {\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "\n",
        "''' Stratified K-Fold is a type of cross-validation object in scikit-learn.\n",
        " It provides train/test indices to split data into train/test sets in a stratified fashion.\n",
        " It is beneficial for imbalanced datasets\n",
        " as it ensures that relative class frequencies are approximately preserved in each train and test set.'''\n",
        "\n",
        "strat_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Use the object in the RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator = brfc,\n",
        "    param_distributions=param_grid,\n",
        "    scoring=scoring,\n",
        "    refit='f1',  # because we are interested in maximizing f1_score\n",
        "    cv=strat_kfold,\n",
        "    n_jobs=1,\n",
        "    verbose=0,\n",
        "    n_iter=10,  # number of parameter settings that are sampled\n",
        "    random_state=42  # for reproducibility\n",
        ")\n",
        "\n",
        "'''# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    estimator = brfc,\n",
        "    param_grid=param_grid,\n",
        "    scoring=scoring,\n",
        "    refit='f1',  # because we are interested in maximizing f1_score\n",
        "    cv=5,\n",
        "    n_jobs=19,\n",
        "    verbose=0\n",
        ")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjrSRflbc4SY",
        "outputId": "97e5db8a-cd9f-49ba-d8b8-b6b242e23b69"
      },
      "outputs": [],
      "source": [
        "# Fit RandomizedSearchCV to the BalancedRandomForestClassifier data\n",
        "#grid_search.fit(X_train, y_train)\n",
        "random_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPjEIEzAqX8E"
      },
      "outputs": [],
      "source": [
        "joblib.dump(random_search, 'random_search_results.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxAxILXMoGI8"
      },
      "outputs": [],
      "source": [
        "# Print all available attributes and methods for the random_search object\n",
        "all_attributes_methods = dir(random_search)\n",
        "\n",
        "# Filter out attributes and methods inherited from BaseSearchCV\n",
        "specific_attributes_methods = [\n",
        "    attribute for attribute in all_attributes_methods\n",
        "    if attribute not in dir(RandomizedSearchCV)\n",
        "]\n",
        "\n",
        "print(\"Attributes and methods specific to GridSearchCV:\")\n",
        "for attr in specific_attributes_methods:\n",
        "    print(attr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUiWyt8ppRRB"
      },
      "outputs": [],
      "source": [
        "def is_fitted(estimator):\n",
        "    try:\n",
        "        getattr(estimator, \"estimators_\")\n",
        "        return True\n",
        "    except AttributeError:\n",
        "        return False\n",
        "\n",
        "print(is_fitted(brfc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiPIXGS9pexb"
      },
      "outputs": [],
      "source": [
        "random_search.score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BppBqD9yp_IH"
      },
      "outputs": [],
      "source": [
        "# Get the best parameters and the corresponding score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "best_estimator = random_search.best_estimator_\n",
        "\n",
        "cv_results = random_search.cv_results_\n",
        "\n",
        "cv_results_df = pd.DataFrame(random_search.cv_results_)\n",
        "\n",
        "scorer = random_search.scorer_\n",
        "\n",
        "refit_time = random_search.refit_time_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFlDh4suqIMz"
      },
      "outputs": [],
      "source": [
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best cross-validation score:\", best_score)\n",
        "print(\"Best estimator:\", best_estimator)\n",
        "print(\"CV Results:\",cv_results_df)\n",
        "print(\"Scorer function:\", scorer)\n",
        "print(\"Refit time (seconds):\", refit_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIReOCjyqQYd"
      },
      "outputs": [],
      "source": [
        "best_model = random_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoaLOoNRqYCm"
      },
      "outputs": [],
      "source": [
        "# Predictions for test data\n",
        "y_pred = best_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8sUbWx-mqkcI"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate F1-score (use 'weighted' or 'macro' depending on your problem)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9xLCpr1qybG"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKIamNLLq1tw"
      },
      "outputs": [],
      "source": [
        "# Predictions for train data\n",
        "y_pred_train = best_model.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENy_x5Tsq685"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and classification report for train data\n",
        "train_cm = confusion_matrix(y_train, y_pred_train)\n",
        "train_cr = classification_report(y_train, y_pred_train)\n",
        "print(\"Training confusion matrix:\")\n",
        "print(train_cm)\n",
        "print(\"Training classification report:\")\n",
        "print(train_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ipc6B6ljrF5l"
      },
      "outputs": [],
      "source": [
        "disp = ConfusionMatrixDisplay.from_estimator(\n",
        "        brfc,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        cmap=plt.cm.Blues)\n",
        "\n",
        "title = disp.ax_.set_title(\"Confusion matrix\")\n",
        "\n",
        "print(title)\n",
        "print(disp.confusion_matrix)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BdV08TIroRy"
      },
      "outputs": [],
      "source": [
        " Calculate feature importances and the standard deviation for those importances\n",
        "importances = best_model.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in best_model.estimators_], axis=0)\n",
        "\n",
        "\n",
        " # list of feature names corresponding to the input bands of your raster stack\n",
        "feature_names =  [ 'SOIL', 'ROAD', 'LUP_10', 'PRECIPITATION', 'RIVER', 'CITIES', 'PORTS' ]\n",
        "# Create a sorted list of tuples containing feature names and their importances:\n",
        "sorted_features = sorted(zip(feature_names, importances, std), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Create a bar chart\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Set the feature names as x-axis labels\n",
        "ax.set_xticklabels([item[0] for item in sorted_features], rotation=45, ha='right')\n",
        "ax.set_xticks(range(len(sorted_features)))\n",
        "\n",
        "# Set the y-axis labels as importances\n",
        "ax.bar(range(len(sorted_features)), [item[1] for item in sorted_features], yerr=[item[2] for item in sorted_features])\n",
        "\n",
        "# Set the title and labels for the chart\n",
        "ax.set_title('Feature Importances')\n",
        "ax.set_xlabel('Features')\n",
        "ax.set_ylabel('Importance')\n",
        "\n",
        "# Display the chart\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pvJs3GhtnmO"
      },
      "outputs": [],
      "source": [
        "y_proba_curve = best_model.predict_proba(X_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRk8qxuZu1mR"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of y_proba_curve:\", y_proba_curve.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2OquYbBuIg4"
      },
      "outputs": [],
      "source": [
        "# Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_proba_curve)\n",
        "plt.plot(recall, precision, marker='.', label='Random Forest')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Area under Precision-Recall curve: {auc(recall, precision)}\")\n",
        "\n",
        "# ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba_curve)\n",
        "plt.plot(fpr, tpr, marker='.', label='Random Forest')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Area under ROC curve: {auc(fpr, tpr)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X3Gu9A7vmZ9"
      },
      "outputs": [],
      "source": [
        "# Predict probabilities for deforestation events\n",
        "y_proba = best_model.predict_proba(X_cleaned)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3OVdc6Rxazr",
        "outputId": "fc6e66a3-a95a-475d-ba18-0ef7bdf5bdbd"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Predicts the\n",
        "# Create a probability raster by filling in the valid pixel values\n",
        "prob_raster = np.full(y.shape, no_data_value, dtype=np.float32)\n",
        "prob_raster[valid_rows] = y_proba\n",
        "prob_raster = prob_raster.reshape(feature_data_arrays[0].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH-I5xH0x42d"
      },
      "outputs": [],
      "source": [
        "print(y_proba.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXYpYrfykVFZ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    joblib.dump(best_params, 'best_params.pkl')\n",
        "    joblib.dump(best_score, 'best_score.pkl')\n",
        "    joblib.dump(best_model, 'best_model.pkl')\n",
        "    joblib.dump(cv_results, 'cv_results.pkl')\n",
        "    joblib.dump(cv_results_df, 'cv_results_df.pkl')\n",
        "    joblib.dump(scorer, 'scorer.pkl')\n",
        "    joblib.dump(refit_time, 'refit_time.pkl')\n",
        "    joblib.dump(report, 'report.pkl')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu_Y-p_rkYPq"
      },
      "outputs": [],
      "source": [
        "# Save the probability raster as a GeoTIFF file\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "output_file = os.path.join(output_folder, \"brfc-df-prediction-feature.tiff\")\n",
        "\n",
        "with rasterio.open(y_file) as src:\n",
        "    profile = src.profile\n",
        "    profile.update(dtype=rasterio.float32, count=1)\n",
        "\n",
        "prob_raster_reshaped = prob_raster.reshape((1, prob_raster.shape[0], prob_raster.shape[1]))\n",
        "\n",
        "with rasterio.open(output_file, 'w', **profile) as dst:\n",
        "    dst.write_band(1, prob_raster_reshaped[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iRMb_cgkb90"
      },
      "outputs": [],
      "source": [
        "# Report\n",
        "model_report = f'''\n",
        "\n",
        "Balanced Random Forest Classifier Model Report\n",
        "\n",
        "# Summary\n",
        "\n",
        "The Balanced Random Forest Classifier performed reasonably well on this task,\n",
        "with an accuracy of  {accuracy} and an F1-score of {f1}.\n",
        "However, there is room for improvement, particularly in the precision and recall for class 1.\n",
        "Future work could explore different models, additional feature engineering, or further hyperparameter tuning to improve performance.\n",
        "\n",
        "# Model Selection\n",
        "\n",
        "We chose to use a Balanced Random Forest Classifier for this task.\n",
        "This model is an ensemble method that combines the predictions of several base estimators\n",
        "built with a given learning algorithm in order to improve generalizability and robustness over a single estimator.\n",
        "It also handles imbalanced classes, which is a common problem in many machine learning tasks.\n",
        "\n",
        "Hyperparameter Tuning\n",
        "We used RandomizedSearchCV for hyperparameter tuning.\n",
        "This method performs a random search on hyperparameters, which is more efficient than an exhaustive search like GridSearchCV.\n",
        "\n",
        "The hyperparameters we tuned were:\n",
        "\n",
        "'n_estimators': The number of trees in the forest.\n",
        "'max_depth': The maximum depth of the tree.\n",
        "'min_samples_split': The minimum number of samples required to split a node.\n",
        "'min_samples_leaf': The minimum number of samples required at a leaf node.\n",
        "'bootstrap': Whether bootstrap samples are used when building trees.\n",
        "\n",
        "{param_grid}\n",
        "\n",
        "# Model Performance\n",
        "The best parameters found by RandomizedSearchCV were:\n",
        "\n",
        "Best parameters:, {best_params}\n",
        "\n",
        "\n",
        "\n",
        "With these parameters, the model achieved the following performance metrics:\n",
        "Best cross-validation score: {best_score}\n",
        "Best model:, {best_estimator}\n",
        "Scorer function:, {scorer}\n",
        "Refit time (seconds): {refit_time}\n",
        "Accuracy:, {accuracy}\n",
        "F1-score: {f1}\n",
        "\n",
        "# Testing Data\n",
        "\n",
        "Classification report:\n",
        "\n",
        "{report}\n",
        "\n",
        "#  TRAINING DATA Classificatin Report-Confusion Matrix\n",
        "\n",
        "Training confusion matrix:\n",
        "\n",
        "{train_cm}\n",
        "\n",
        "Training classification report:\n",
        "\n",
        "{train_cr}\n",
        "\n",
        "\n",
        "This indicates that the model correctly classified [1,1] instances of class 0\n",
        "and [2,2] instances of class 1,\n",
        "\n",
        "while misclassifying [1,2] instances of class 0 and [2,1] instances of class 1.\n",
        "\n",
        "CV Results:\n",
        "{cv_results_df}\n",
        "\n",
        "'''\n",
        "# Write the report to a Quarto markdown file\n",
        "with open('model_report.qmd', 'w') as f:\n",
        "    f.write(model_report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
