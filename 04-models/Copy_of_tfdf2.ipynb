{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kztx6LuVVWBX"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWkvr-Goe7vy"
      },
      "outputs": [],
      "source": [
        "\n",
        "import rasterio\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from joblib import dump\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1DoTLm6WZdm"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import (confusion_matrix, classification_report, accuracy_score,\n",
        "                             precision_score, recall_score, f1_score, roc_auc_score,\n",
        "                             precision_recall_curve, roc_curve, auc)\n",
        "from sklearn.model_selection import (train_test_split, cross_val_score, RandomizedSearchCV)\n",
        "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n",
        "from scipy.stats import randint as sp_randint\n",
        "\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "\n",
        "from scipy.stats import skew\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owrViUXGZ7Hx"
      },
      "outputs": [],
      "source": [
        "FEATURES_DIR = '/Users/guillermoromero/Documents/post-meds/data/policy-data/training'\n",
        "EXCLUDE_FILE = 'train_binary_deforestation_raster.tif'\n",
        "\n",
        "# Path to the y_file\n",
        "y_file = os.path.join(FEATURES_DIR, 'train_binary_deforestation_raster.tif')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to read TIFF files\n",
        "def read_tiff_image(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        return src.read(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFaLv6mJKtrJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# List of paths to the raster files excluding the specified file\n",
        "feature_files = [os.path.join(FEATURES_DIR, file_name)\n",
        "                 for file_name in os.listdir(FEATURES_DIR)\n",
        "                 if file_name != EXCLUDE_FILE]\n",
        "\n",
        "# Read and store each raster file's data in an array\n",
        "#feature_data_arrays = [read_tiff_image(file_path) for file_path in feature_files]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0lhZKxqjVpi",
        "outputId": "bf1728a8-1bcf-4468-cce1-3bcf3184ac9b"
      },
      "outputs": [],
      "source": [
        "feature_files = [os.path.join(FEATURES_DIR, file_name)\n",
        "                 for file_name in os.listdir(FEATURES_DIR)\n",
        "                 if file_name != EXCLUDE_FILE]\n",
        "feature_files.sort()\n",
        "\n",
        "feature_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwXMBLpJc1CE"
      },
      "outputs": [],
      "source": [
        "# Read the feature rasters and stack them into a single array\n",
        "X = np.stack([read_tiff_image(file_path) for file_path in feature_files])\n",
        "\n",
        "# Read the target raster\n",
        "with rasterio.open(y_file) as src:\n",
        "    y = src.read(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dmiUDScc725"
      },
      "outputs": [],
      "source": [
        "# Flatten the arrays and remove nodata values\n",
        "nodata_mask = np.any(X == -1, axis=0)\n",
        "X = X[:, ~nodata_mask].T\n",
        "y = y[~nodata_mask]\n",
        "\n",
        "# Define the indices of the categorical features (grupo and soil)\n",
        "grupo_index = feature_files.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/training/train_grupo_masked.tif')\n",
        "soil_index = feature_files.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/training/train_soil_masked.tif')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the unique grupo and soil values from the training data\n",
        "unique_grupo_train = np.unique(X[:, grupo_index])\n",
        "unique_soil_train = np.unique(X[:, soil_index])\n",
        "print(unique_grupo_train)\n",
        "print(unique_soil_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the indices of the features that need log transformation (river, cities, roads)\n",
        "log_indices = [\n",
        "    feature_files.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/training/train_river_distance_raster.tif'),\n",
        "    feature_files.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/training/train_cities_masked.tif'),\n",
        "    feature_files.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/training/train_road_distance_raster.tif')\n",
        "]\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "preprocessor = make_column_transformer(\n",
        "    (OneHotEncoder(), [grupo_index, soil_index]),  # One-hot encode categorical features\n",
        "    (FunctionTransformer(np.log1p), log_indices),  # Apply log transformation\n",
        "    remainder='passthrough'  # Pass the remaining features as is\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhc8_e_EcQLt"
      },
      "outputs": [],
      "source": [
        "# Apply the preprocessing steps to the entire dataset\n",
        "X_preprocessed = preprocessor.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raiosNIdQy3z"
      },
      "outputs": [],
      "source": [
        "del X\n",
        "del feature_files\n",
        "#del feature_data_arrays\n",
        "del grupo_index\n",
        "del soil_index\n",
        "del log_indices\n",
        "del preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cO_Pd01uNN2r"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, train_size =0.15, stratify=y, random_state=42)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDD_sLM3RWvS"
      },
      "outputs": [],
      "source": [
        "del X_preprocessed\n",
        "del y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwpuNiwcNc77",
        "outputId": "6bc7fab8-8393-4e63-c790-33f27dbe5f9b"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgP7PF4-dgBH",
        "outputId": "91878d1e-78aa-466d-fc25-3404503b2e6c"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.parallel import delayed, Parallel\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Function delayed is deprecated\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`sklearn.utils.parallel.delayed` should be used with `sklearn.utils.parallel.Parallel`\")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Define the updated parameter grid\n",
        "param_grid = {\n",
        "    'balancedrandomforestclassifier__n_estimators': sp_randint(100, 1000),\n",
        "    'balancedrandomforestclassifier__max_depth': [None] + list(range(10, 51)),\n",
        "    'balancedrandomforestclassifier__min_samples_split': sp_randint(2, 21),\n",
        "    'balancedrandomforestclassifier__min_samples_leaf': sp_randint(1, 21),\n",
        "    'balancedrandomforestclassifier__max_features': ['sqrt', 'log2', None] + list(np.arange(0.2, 1.0, 0.1)),\n",
        "    'balancedrandomforestclassifier__criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Compute class weights on the full training set or a large enough sample\n",
        "sample_size = min(len(y_train), 10000)  # Adjust the sample size as needed\n",
        "sample_indices = np.random.choice(len(y_train), size=sample_size, replace=False)\n",
        "sample_y = y_train[sample_indices]\n",
        "\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(sample_y), y=sample_y)\n",
        "class_weight_dict = dict(zip(np.unique(sample_y), class_weights))\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = make_pipeline(\n",
        "    BalancedRandomForestClassifier(random_state=42, class_weight=class_weight_dict, warm_start=True, bootstrap=False, replacement=True)\n",
        ")\n",
        "\n",
        "# Initialize the number of iterations and data size\n",
        "n_iter = 50\n",
        "data_size = 2000\n",
        "\n",
        "# Create the StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Set scoring metrics\n",
        "scoring = {\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "# Iterate and narrow down the grid\n",
        "for _ in range(3):  # Number of iterations to narrow down the grid\n",
        "    # Sample a subset of data for the current iteration\n",
        "    idx = np.random.choice(X_train.shape[0], size=data_size, replace=False)\n",
        "    X_subset = X_train[idx]\n",
        "    y_subset = y_train[idx]\n",
        "\n",
        "    # Create the RandomizedSearchCV object\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=n_iter,\n",
        "        cv=skf,\n",
        "        scoring=scoring,\n",
        "        refit='f1',\n",
        "        n_jobs=1,\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Fit the RandomizedSearchCV object on the subset of data\n",
        "    random_search.fit(X_subset, y_subset)\n",
        "\n",
        "    # Get the best parameters and best score for the current iteration\n",
        "    best_params = random_search.best_params_\n",
        "    best_score = random_search.best_score_\n",
        "    best_estimator = random_search.best_estimator_\n",
        "\n",
        "    print(f\"Iteration best parameters: {best_params}\")\n",
        "    print(f\"Iteration best score: {best_score}\")\n",
        "    print(f\"Iteration best estimator: {best_estimator}\")\n",
        "\n",
        "    # Update the parameter grid based on the best parameters\n",
        "    if isinstance(best_params['balancedrandomforestclassifier__max_features'], str):\n",
        "      max_features_values = ['sqrt', 'log2', None]\n",
        "    else:\n",
        "      max_features_values = list(np.arange(max(0.1, float(best_params['balancedrandomforestclassifier__max_features']) - 0.2), min(1.0, float(best_params['balancedrandomforestclassifier__max_features']) + 0.2), 0.1))\n",
        "\n",
        "      # Update the parameter grid based on the best parameters\n",
        "    param_grid = {\n",
        "        'balancedrandomforestclassifier__n_estimators': sp_randint(max(100, best_params['balancedrandomforestclassifier__n_estimators'] - 100), best_params['balancedrandomforestclassifier__n_estimators'] + 100),\n",
        "        'balancedrandomforestclassifier__max_depth': [best_params['balancedrandomforestclassifier__max_depth']] + list(range(max(10, best_params['balancedrandomforestclassifier__max_depth'] - 10), min(51, best_params['balancedrandomforestclassifier__max_depth'] + 11))),\n",
        "        'balancedrandomforestclassifier__min_samples_split': sp_randint(max(2, best_params['balancedrandomforestclassifier__min_samples_split'] - 4), best_params['balancedrandomforestclassifier__min_samples_split'] + 5),\n",
        "        'balancedrandomforestclassifier__min_samples_leaf': sp_randint(max(1, best_params['balancedrandomforestclassifier__min_samples_leaf'] - 4), best_params['balancedrandomforestclassifier__min_samples_leaf'] + 5),\n",
        "        'balancedrandomforestclassifier__max_features': max_features_values,\n",
        "        'balancedrandomforestclassifier__criterion': ['gini', 'entropy']\n",
        "    }\n",
        "\n",
        "    # Increase the data size for the next iteration\n",
        "    data_size *= 2\n",
        "\n",
        "# Get the overall best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "best_estimator = random_search.best_estimator_\n",
        "print(\"Overall best parameters: \", best_params)\n",
        "print(\"Overall best score: \", best_score)\n",
        "print(\"Overall best estimator: \", best_estimator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBD_EZFz0yRR",
        "outputId": "e9722e58-4e0d-4595-e547-bfd9a1ea3da8"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.parallel import delayed, Parallel\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Function delayed is deprecated\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`sklearn.utils.parallel.delayed` should be used with `sklearn.utils.parallel.Parallel`\")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Define the updated parameter grid\n",
        "param_grid = {\n",
        "    'balancedrandomforestclassifier__n_estimators': sp_randint(100, 1000),\n",
        "    'balancedrandomforestclassifier__max_depth': [None] + list(range(10, 51)),\n",
        "    'balancedrandomforestclassifier__min_samples_split': sp_randint(2, 21),\n",
        "    'balancedrandomforestclassifier__min_samples_leaf': sp_randint(1, 21),\n",
        "    'balancedrandomforestclassifier__max_features': ['sqrt', 'log2', None] + list(np.arange(0.2, 1.0, 0.1)),\n",
        "    'balancedrandomforestclassifier__criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "# Compute class weights on the full training set or a large enough sample\n",
        "sample_size = min(len(y_train), 10000)  # Adjust the sample size as needed\n",
        "sample_indices = np.random.choice(len(y_train), size=sample_size, replace=False)\n",
        "sample_y = y_train[sample_indices]\n",
        "\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(sample_y), y=sample_y)\n",
        "class_weight_dict = dict(zip(np.unique(sample_y), class_weights))\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = make_pipeline(\n",
        "    BalancedRandomForestClassifier(random_state=42, class_weight=class_weight_dict, warm_start=True, bootstrap=False, replacement=True)\n",
        ")\n",
        "\n",
        "# Initialize the number of iterations and data size\n",
        "n_iter = 50\n",
        "data_size = 10000\n",
        "\n",
        "# Create the StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Set scoring metrics\n",
        "scoring = {\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "# Iterate and narrow down the grid\n",
        "for _ in range(3):  # Number of iterations to narrow down the grid\n",
        "    # Sample a subset of data for the current iteration\n",
        "    idx = np.random.choice(X_train.shape[0], size=data_size, replace=False)\n",
        "    X_subset = X_train[idx]\n",
        "    y_subset = y_train[idx]\n",
        "\n",
        "    # Create the RandomizedSearchCV object\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=n_iter,\n",
        "        cv=skf,\n",
        "        scoring=scoring,\n",
        "        refit='f1',\n",
        "        n_jobs=1,\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Fit the RandomizedSearchCV object on the subset of data\n",
        "    random_search.fit(X_subset, y_subset)\n",
        "\n",
        "    # Get the best parameters and best score for the current iteration\n",
        "    best_params = random_search.best_params_\n",
        "    best_score = random_search.best_score_\n",
        "    best_estimator = random_search.best_estimator_\n",
        "\n",
        "    print(f\"Iteration best parameters: {best_params}\")\n",
        "    print(f\"Iteration best score: {best_score}\")\n",
        "    print(f\"Iteration best estimator: {best_estimator}\")\n",
        "\n",
        "    # Update the parameter grid based on the best parameters\n",
        "    if isinstance(best_params['balancedrandomforestclassifier__max_features'], str):\n",
        "      max_features_values = ['sqrt', 'log2', None]\n",
        "    else:\n",
        "      max_features_values = list(np.arange(max(0.1, float(best_params['balancedrandomforestclassifier__max_features']) - 0.2), min(1.0, float(best_params['balancedrandomforestclassifier__max_features']) + 0.2), 0.1))\n",
        "\n",
        "      # Update the parameter grid based on the best parameters\n",
        "    param_grid = {\n",
        "        'balancedrandomforestclassifier__n_estimators': sp_randint(max(100, best_params['balancedrandomforestclassifier__n_estimators'] - 100), best_params['balancedrandomforestclassifier__n_estimators'] + 100),\n",
        "        'balancedrandomforestclassifier__max_depth': [best_params['balancedrandomforestclassifier__max_depth']] + list(range(max(10, best_params['balancedrandomforestclassifier__max_depth'] - 10), min(51, best_params['balancedrandomforestclassifier__max_depth'] + 11))),\n",
        "        'balancedrandomforestclassifier__min_samples_split': sp_randint(max(2, best_params['balancedrandomforestclassifier__min_samples_split'] - 4), best_params['balancedrandomforestclassifier__min_samples_split'] + 5),\n",
        "        'balancedrandomforestclassifier__min_samples_leaf': sp_randint(max(1, best_params['balancedrandomforestclassifier__min_samples_leaf'] - 4), best_params['balancedrandomforestclassifier__min_samples_leaf'] + 5),\n",
        "        'balancedrandomforestclassifier__max_features': max_features_values,\n",
        "        'balancedrandomforestclassifier__criterion': ['gini', 'entropy']\n",
        "    }\n",
        "\n",
        "    # Increase the data size for the next iteration\n",
        "    data_size *= 2\n",
        "\n",
        "# Get the overall best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "best_estimator = random_search.best_estimator_\n",
        "print(\"Overall best parameters: \", best_params)\n",
        "print(\"Overall best score: \", best_score)\n",
        "print(\"Overall best estimator: \", best_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKIubf20c2XS",
        "outputId": "56b21119-2f05-48a8-f953-b1e57d11b752"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.parallel import delayed, Parallel\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Function delayed is deprecated\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`sklearn.utils.parallel.delayed` should be used with `sklearn.utils.parallel.Parallel`\")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'balancedrandomforestclassifier__n_estimators': sp_randint(500, 700),\n",
        "    'balancedrandomforestclassifier__max_depth': list(range(30, 41)),\n",
        "    'balancedrandomforestclassifier__min_samples_split': [2, 3, 4],\n",
        "    'balancedrandomforestclassifier__min_samples_leaf': [1, 2, 3],\n",
        "    'balancedrandomforestclassifier__max_features': list(np.arange(0.3, 0.6, 0.05)),\n",
        "    'balancedrandomforestclassifier__criterion': ['gini', 'entropy'],\n",
        "    'balancedrandomforestclassifier__bootstrap': [True, False]\n",
        "}\n",
        "# Compute class weights on the full training set or a large enough sample\n",
        "sample_size = min(len(y_train), 10000)  # Adjust the sample size as needed\n",
        "sample_indices = np.random.choice(len(y_train), size=sample_size, replace=False)\n",
        "sample_y = y_train[sample_indices]\n",
        "\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(sample_y), y=sample_y)\n",
        "class_weight_dict = dict(zip(np.unique(sample_y), class_weights))\n",
        "\n",
        "# Create the pipeline\n",
        "pipeline = make_pipeline(\n",
        "    BalancedRandomForestClassifier(random_state=42, class_weight=class_weight_dict, warm_start=True, bootstrap=False, replacement=True)\n",
        ")\n",
        "\n",
        "# Initialize the number of iterations and data size\n",
        "n_iter = 50\n",
        "data_size = 10000\n",
        "\n",
        "# Create the StratifiedKFold object\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "\n",
        "# Set scoring metrics\n",
        "scoring = {\n",
        "    'precision': 'precision',\n",
        "    'recall': 'recall',\n",
        "    'f1': 'f1',\n",
        "    'roc_auc': 'roc_auc'\n",
        "}\n",
        "\n",
        "# Iterate and narrow down the grid\n",
        "for _ in range(3):  # Number of iterations to narrow down the grid\n",
        "    # Sample a subset of data for the current iteration\n",
        "    idx = np.random.choice(X_train.shape[0], size=data_size, replace=False)\n",
        "    X_subset = X_train[idx]\n",
        "    y_subset = y_train[idx]\n",
        "\n",
        "    # Create the RandomizedSearchCV object\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=pipeline,\n",
        "        param_distributions=param_grid,\n",
        "        n_iter=n_iter,\n",
        "        cv=skf,\n",
        "        scoring=scoring,\n",
        "        refit='f1',\n",
        "        n_jobs=1,\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Fit the RandomizedSearchCV object on the subset of data\n",
        "    random_search.fit(X_subset, y_subset)\n",
        "\n",
        "    # Get the best parameters and best score for the current iteration\n",
        "    best_params = random_search.best_params_\n",
        "    best_score = random_search.best_score_\n",
        "    best_estimator = random_search.best_estimator_\n",
        "\n",
        "    print(f\"Iteration best parameters: {best_params}\")\n",
        "    print(f\"Iteration best score: {best_score}\")\n",
        "    print(f\"Iteration best estimator: {best_estimator}\")\n",
        "\n",
        "    # Update the parameter grid based on the best parameters\n",
        "    if isinstance(best_params['balancedrandomforestclassifier__max_features'], str):\n",
        "      max_features_values = ['sqrt', 'log2', None]\n",
        "    else:\n",
        "      max_features_values = list(np.arange(max(0.1, float(best_params['balancedrandomforestclassifier__max_features']) - 0.2), min(1.0, float(best_params['balancedrandomforestclassifier__max_features']) + 0.2), 0.1))\n",
        "\n",
        "      # Update the parameter grid based on the best parameters\n",
        "    param_grid = {\n",
        "      'balancedrandomforestclassifier__n_estimators': sp_randint(500, 700),\n",
        "      'balancedrandomforestclassifier__max_depth': list(range(30, 41)),\n",
        "      'balancedrandomforestclassifier__min_samples_split': [2, 3, 4],\n",
        "      'balancedrandomforestclassifier__min_samples_leaf': [1, 2, 3],\n",
        "      'balancedrandomforestclassifier__max_features': list(np.arange(0.3, 0.6, 0.05)),\n",
        "      'balancedrandomforestclassifier__criterion': ['gini', 'entropy'],\n",
        "      'balancedrandomforestclassifier__bootstrap': [True, False]\n",
        "      }\n",
        "\n",
        "    # Increase the data size for the next iteration\n",
        "    data_size *= 2\n",
        "\n",
        "# Get the overall best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "best_estimator = random_search.best_estimator_\n",
        "print(\"Overall best parameters: \", best_params)\n",
        "print(\"Overall best score: \", best_score)\n",
        "print(\"Overall best estimator: \", best_estimator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKGxg2C2X45N",
        "outputId": "b0506147-0d1f-4599-e1e5-ff77579a8838"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.parallel import delayed, Parallel\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\"Function delayed is deprecated\")\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"`sklearn.utils.parallel.delayed` should be used with `sklearn.utils.parallel.Parallel`\")\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the classifier with the best parameters\n",
        "brfc = BalancedRandomForestClassifier(\n",
        "    n_estimators=50,\n",
        "    max_depth=36,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features=0.4,\n",
        "    bootstrap=False,\n",
        "    replacement=True,\n",
        "    criterion='entropy',\n",
        "    sampling_strategy='all',\n",
        "    class_weight={0: 0.703037120359955, 1: 1.7313019390581716},\n",
        "    random_state=42,\n",
        "    verbose=2,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "brfc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Export the brfc object\n",
        "joblib.dump(brfc, 'brfc_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "del X_train,  X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5wAItGCn1eB"
      },
      "outputs": [],
      "source": [
        "# Evaluate the classifier on the test data\n",
        "score = brfc.score(X_test, y_test)\n",
        "print(f\"Model accuracy on test data: {score}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxAxILXMoGI8",
        "outputId": "81d13416-c288-449e-d8f0-7df3527c78ea"
      },
      "outputs": [],
      "source": [
        "# Print all available attributes and methods for the random_search object\n",
        "all_attributes_methods = dir(random_search)\n",
        "\n",
        "# Filter out attributes and methods inherited from BaseSearchCV\n",
        "specific_attributes_methods = [\n",
        "    attribute for attribute in all_attributes_methods\n",
        "    if attribute not in dir(RandomizedSearchCV)\n",
        "]\n",
        "\n",
        "print(\"Attributes and methods specific to GridSearchCV:\")\n",
        "for attr in specific_attributes_methods:\n",
        "    print(attr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUiWyt8ppRRB",
        "outputId": "80276096-ef97-409b-c423-d28cb350b593"
      },
      "outputs": [],
      "source": [
        "def is_fitted(estimator):\n",
        "    try:\n",
        "        getattr(estimator, \"estimator\")\n",
        "        return True\n",
        "    except AttributeError:\n",
        "        return False\n",
        "\n",
        "print(is_fitted(brfc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "TiPIXGS9pexb",
        "outputId": "8d1cc6b6-60f7-4189-fab7-db975f05f435"
      },
      "outputs": [],
      "source": [
        "random_search.score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1R8K8tAzTYBW",
        "outputId": "41dee5d7-0c82-488a-bd0e-63311416a6b4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BppBqD9yp_IH"
      },
      "outputs": [],
      "source": [
        "# Get the best parameters and the corresponding score\n",
        "best_params = brfc.best_params_\n",
        "best_score = brfc.best_score_\n",
        "\n",
        "best_estimator = brfc.best_estimator_\n",
        "\n",
        "cv_results = brfc.cv_results_\n",
        "\n",
        "cv_results_df = pd.DataFrame(brfc.cv_results_)\n",
        "\n",
        "scorer = brfc.scorer_\n",
        "\n",
        "refit_time = brfc.refit_time_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFlDh4suqIMz",
        "outputId": "5b8b752a-c439-4353-9621-49f132b93168"
      },
      "outputs": [],
      "source": [
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best cross-validation score:\", best_score)\n",
        "print(\"Best estimator:\", best_estimator)\n",
        "print(\"CV Results:\",cv_results_df)\n",
        "print(\"Scorer function:\", scorer)\n",
        "print(\"Refit time (seconds):\", refit_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIReOCjyqQYd"
      },
      "outputs": [],
      "source": [
        "best_model = random_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG7T0lBwXi3q"
      },
      "outputs": [],
      "source": [
        "f1_score?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "BoaLOoNRqYCm",
        "outputId": "2e299a00-46ed-41ea-dd59-723ecc075533"
      },
      "outputs": [],
      "source": [
        "# Predictions for test data\n",
        "y_pred = brfc.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8sUbWx-mqkcI",
        "outputId": "cc9720a8-b4f7-41f5-aeef-75bfa325c392"
      },
      "outputs": [],
      "source": [
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate F1-score (use 'weighted' or 'macro' depending on your problem)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Print classification report\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(\"Classification report:\\n\", report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh0mzEToXMss"
      },
      "outputs": [],
      "source": [
        "f1_score?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D9xLCpr1qybG",
        "outputId": "8f678f80-b76e-4032-9f53-3e52a7c8433a"
      },
      "outputs": [],
      "source": [
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "uKIamNLLq1tw",
        "outputId": "b3d64c35-6dd4-41cd-ea76-77a25d1b7850"
      },
      "outputs": [],
      "source": [
        "# Predictions for train data\n",
        "y_pred_train = brfc.predict(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENy_x5Tsq685",
        "outputId": "fe938924-86b8-4596-cdb2-1604489f4eb0"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix and classification report for train data\n",
        "train_cm = confusion_matrix(y_train, y_pred_train)\n",
        "train_cr = classification_report(y_train, y_pred_train)\n",
        "print(\"Training confusion matrix:\")\n",
        "print(train_cm)\n",
        "print(\"Training classification report:\")\n",
        "print(train_cr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "Ipc6B6ljrF5l",
        "outputId": "301a77ae-dec4-4722-8562-229a7f8fe06c"
      },
      "outputs": [],
      "source": [
        "disp = ConfusionMatrixDisplay.from_estimator(\n",
        "        brfc,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        cmap=plt.cm.Blues)\n",
        "\n",
        "title = disp.ax_.set_title(\"Confusion matrix\")\n",
        "\n",
        "print(title)\n",
        "print(disp.confusion_matrix)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "6BdV08TIroRy",
        "outputId": "3f200d57-06d6-4a52-8bf9-304d2ebeb1ae"
      },
      "outputs": [],
      "source": [
        " #Calculate feature importances and the standard deviation for those importances\n",
        "importances = brfc.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in brfc.estimators_], axis=0)\n",
        "\n",
        "\n",
        " # list of feature names corresponding to the input bands of your raster stack\n",
        "feature_names =  [ 'CITIES','GRUPO', 'PORTS', 'PRECIPITATION', 'RIVER','ROAD','SOIL'  ]\n",
        "# Create a sorted list of tuples containing feature names and their importances:\n",
        "sorted_features = sorted(zip(feature_names, importances, std), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Create a bar chart\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Set the feature names as x-axis labels\n",
        "ax.set_xticklabels([item[0] for item in sorted_features], rotation=45, ha='right')\n",
        "ax.set_xticks(range(len(sorted_features)))\n",
        "\n",
        "# Set the y-axis labels as importances\n",
        "ax.bar(range(len(sorted_features)), [item[1] for item in sorted_features], yerr=[item[2] for item in sorted_features])\n",
        "\n",
        "# Set the title and labels for the chart\n",
        "ax.set_title('Feature Importances')\n",
        "ax.set_xlabel('Features')\n",
        "ax.set_ylabel('Importance')\n",
        "\n",
        "# Display the chart\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "S_YAdmdQVZui",
        "outputId": "8baaa16c-3319-4aa4-89a3-cc37c6050e47"
      },
      "outputs": [],
      "source": [
        "# Assuming you have trained your model and obtained the feature importances\n",
        "importances = brfc.feature_importances_\n",
        "\n",
        "# Get the indices of the GRUPO-related binary features\n",
        "grupo_indices = [i for i, feature in enumerate(feature_names) if 'GRUPO' in feature]\n",
        "\n",
        "# Calculate the aggregated importance score for the GRUPO feature\n",
        "grupo_importance = np.sum(importances[grupo_indices])\n",
        "\n",
        "# Create a new list of feature names with GRUPO as a single feature\n",
        "aggregated_feature_names = [feature for feature in feature_names if 'GRUPO' not in feature]\n",
        "aggregated_feature_names.append('GRUPO')\n",
        "\n",
        "# Create a new list of importances with the aggregated GRUPO importance\n",
        "aggregated_importances = [imp for i, imp in enumerate(importances) if i not in grupo_indices]\n",
        "aggregated_importances.append(grupo_importance)\n",
        "\n",
        "# Create a sorted list of tuples containing aggregated feature names and their importances\n",
        "sorted_features = sorted(zip(aggregated_feature_names, aggregated_importances), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Visualize the aggregated feature importances\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xticklabels([item[0] for item in sorted_features], rotation=45, ha='right')\n",
        "ax.set_xticks(range(len(sorted_features)))\n",
        "ax.bar(range(len(sorted_features)), [item[1] for item in sorted_features])\n",
        "ax.set_title('Aggregated Feature Importances')\n",
        "ax.set_xlabel('Features')\n",
        "ax.set_ylabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YI8zVleuMMs"
      },
      "outputs": [],
      "source": [
        "feature_groups = [['CITIES'], ['PORTS'], ['PRECIPITATION'], ['RIVER'], ['ROAD'], ['SOIL']]\n",
        "grupo_features = [feature for feature in feature_names if 'GRUPO' in feature]\n",
        "feature_groups.append(grupo_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW6xMiPwlWi9"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "# Assuming you have your trained model (brfc) and the preprocessed input data (X_test, y_test)\n",
        "\n",
        "# Convert X_test to a dense numpy array\n",
        "X_test_dense = X_test.toarray()\n",
        "\n",
        "# Initialize a dictionary to store the permutation importances for each feature group\n",
        "perm_importances = {}\n",
        "\n",
        "# Iterate over each feature group\n",
        "for group in feature_groups:\n",
        "    # Calculate the permutation importance for all features\n",
        "    perm_importance = permutation_importance(brfc, X_test_dense, y_test, n_repeats=10, random_state=42)\n",
        "\n",
        "    # Create a boolean mask for the current feature group\n",
        "    mask = [feature in group for feature in feature_names]\n",
        "    mask = np.array(mask)\n",
        "    # Set the importance of non-group features to zero\n",
        "    perm_importance.importances_mean[~mask] = 0\n",
        "    perm_importance.importances_std[~mask] = 0\n",
        "\n",
        "    # Store the mean and standard deviation of the permutation importance scores\n",
        "    perm_importances[group[0] if len(group) == 1 else 'GRUPO'] = (perm_importance.importances_mean.sum(),\n",
        "                                                                  perm_importance.importances_std.sum())\n",
        "\n",
        "# Create a sorted list of tuples containing feature group names and their importances\n",
        "sorted_grouped_features = sorted(perm_importances.items(), key=lambda x: x[1][0], reverse=True)\n",
        "\n",
        "# Visualize the grouped feature importances\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.set_xticklabels([item[0] for item in sorted_grouped_features], rotation=45, ha='right')\n",
        "ax.set_xticks(range(len(sorted_grouped_features)))\n",
        "ax.bar(range(len(sorted_grouped_features)), [item[1][0] for item in sorted_grouped_features],\n",
        "       yerr=[item[1][1] for item in sorted_grouped_features])\n",
        "ax.set_title('Grouped Feature Importances')\n",
        "ax.set_xlabel('Feature Groups')\n",
        "ax.set_ylabel('Permutation Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "5kjog4NPlJPQ",
        "outputId": "b0e0ef1a-1bca-442c-c4c1-bfd0c7fa751f"
      },
      "outputs": [],
      "source": [
        "#AREA_AUTORIZADA: 1 BOSQUES: 2 EN_CONFLICTO: 3 OTRAS_COBERTURAS: 4 OTRAS_TIERRAS_FORESTALES: 5 unclassified: 6\n",
        "\n",
        "# List of feature names corresponding to the input raster stack\n",
        "feature_names = ['CITIES', 'GRUPO_1', 'GRUPO_2', 'GRUPO_3', 'GRUPO_4', 'GRUPO_5', 'GRUPO_6',\n",
        "                 'PORTS', 'PRECIPITATION', 'RIVER', 'ROAD',\n",
        "                 'SOIL_1', 'SOIL_2', 'SOIL_3', 'SOIL_4', 'SOIL_5', 'SOIL_6', 'SOIL_7',\n",
        "                 'SOIL_8', 'SOIL_9', 'SOIL_10', 'SOIL_11', 'SOIL_12', 'SOIL_13',\n",
        "                 'SOIL_14', 'SOIL_15', 'SOIL_16']\n",
        "# Calculate feature importances and the standard deviation for those importances\n",
        "importances = brfc.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in brfc.estimators_], axis=0)\n",
        "\n",
        "# Create a sorted list of tuples containing feature names and their importances\n",
        "sorted_features = sorted(zip(feature_names, importances, std), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Create a bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Set the feature names as x-axis labels\n",
        "ax.set_xticklabels([item[0] for item in sorted_features], rotation=45, ha='right')\n",
        "ax.set_xticks(range(len(sorted_features)))\n",
        "\n",
        "# Set the y-axis labels as importances\n",
        "ax.bar(range(len(sorted_features)), [item[1] for item in sorted_features],\n",
        "       yerr=[item[2] for item in sorted_features])\n",
        "\n",
        "# Set the title and labels for the chart\n",
        "ax.set_title('Feature Importances')\n",
        "ax.set_xlabel('Features')\n",
        "ax.set_ylabel('Importance')\n",
        "\n",
        "# Display the chart\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "aH9dJh-Rlaj5",
        "outputId": "f1e1c9ed-2028-437e-c542-d4583d221fcb"
      },
      "outputs": [],
      "source": [
        "#AREA_AUTORIZADA: 1 BOSQUES: 2 EN_CONFLICTO: 3 OTRAS_COBERTURAS: 4 OTRAS_TIERRAS_FORESTALES: 5 unclassified: 6\n",
        "\n",
        "# List of feature names corresponding to the input raster stack\n",
        "feature_names = ['CITIES', 'GRUPO_1', 'GRUPO_2', 'GRUPO_3', 'GRUPO_4', 'GRUPO_5', 'GRUPO_6',\n",
        "                 'PORTS', 'PRECIPITATION', 'RIVER', 'ROAD',\n",
        "                 'SOIL_1', 'SOIL_2', 'SOIL_3', 'SOIL_4', 'SOIL_5', 'SOIL_6', 'SOIL_7',\n",
        "                 'SOIL_8', 'SOIL_9', 'SOIL_10', 'SOIL_11', 'SOIL_12', 'SOIL_13',\n",
        "                 'SOIL_14', 'SOIL_15', 'SOIL_16']\n",
        "# Calculate feature importances and the standard deviation for those importances\n",
        "importances = brfc.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in brfc.estimators_], axis=0)\n",
        "\n",
        "# Create a sorted list of tuples containing feature names and their importances\n",
        "sorted_features = sorted(zip(feature_names, importances, std), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Create a bar chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Set the feature names as x-axis labels\n",
        "ax.set_xticklabels([item[0] for item in sorted_features], rotation=45, ha='right')\n",
        "ax.set_xticks(range(len(sorted_features)))\n",
        "\n",
        "# Set the y-axis labels as importances\n",
        "ax.bar(range(len(sorted_features)), [item[1] for item in sorted_features],\n",
        "       yerr=[item[2] for item in sorted_features])\n",
        "\n",
        "# Set the title and labels for the chart\n",
        "ax.set_title('Feature Importances')\n",
        "ax.set_xlabel('Features')\n",
        "ax.set_ylabel('Importance')\n",
        "\n",
        "# Display the chart\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5pvJs3GhtnmO"
      },
      "outputs": [],
      "source": [
        "y_pred_prob = brfc.predict_proba(X_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRk8qxuZu1mR",
        "outputId": "3727f20e-4db6-4620-9f2a-8c96e70292a2"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of y_proba_curve:\", y_pred_prob.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h2OquYbBuIg4",
        "outputId": "6d15a8b8-15db-4730-8fa9-6ef23e480526"
      },
      "outputs": [],
      "source": [
        "#Assuming you have the true labels (y_test) and predicted probabilities (y_pred_prob)\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Plot the precision-recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(recall, precision, marker='.', label='Precision-Recall Curve')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Area under Precision-Recall curve: {auc(recall, precision)}\")\n",
        "\n",
        "# Assuming you have the true labels (y_test) and predicted probabilities (y_pred_prob)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Calculate the Area Under the Curve (AUC)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--', label='Random guess')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Area under ROC curve: {auc(fpr, tpr)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Spatial Blocking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "brfc = joblib.load('brfc_model.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VAL_FEATURES_DIR = '/Users/guillermoromero/Documents/post-meds/data/policy-data/validation'\n",
        "VAL_EXCLUDE_FILE = 'val_binary_deforestation_raster.tif'\n",
        "\n",
        "# Path to the y_file\n",
        "y_file_val = os.path.join(VAL_FEATURES_DIR, 'val_binary_deforestation_raster.tif')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X3Gu9A7vmZ9"
      },
      "outputs": [],
      "source": [
        "feature_files_val = [os.path.join(VAL_FEATURES_DIR, file_name)\n",
        "                 for file_name in os.listdir(VAL_FEATURES_DIR)\n",
        "                 if file_name != VAL_EXCLUDE_FILE]\n",
        "feature_files_val.sort()\n",
        "\n",
        "feature_files_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function to read TIFF files\n",
        "def read_tiff_image(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        return src.read(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3OVdc6Rxazr",
        "outputId": "fc6e66a3-a95a-475d-ba18-0ef7bdf5bdbd"
      },
      "outputs": [],
      "source": [
        "# Read the feature rasters and stack them into a single array\n",
        "X = np.stack([read_tiff_image(file_path) for file_path in feature_files_val])\n",
        "\n",
        "# Read the target raster\n",
        "with rasterio.open(y_file_val) as src:\n",
        "    y = src.read(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten the arrays and remove nodata values\n",
        "nodata_mask_val = np.any(X == -1, axis=0)\n",
        "X_val = X[:, ~nodata_mask_val].T\n",
        "y_val = y[~nodata_mask_val]\n",
        "\n",
        "# Define the indices of the categorical features (grupo and soil)\n",
        "grupo_index_val = feature_files_val.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/validation/val_grupo_masked.tif')\n",
        "soil_index_val = feature_files_val.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/validation/val_soil_masked.tif')\n",
        "\n",
        "# Create dummy features for all grupo values\n",
        "grupo_dummies = np.zeros((X_val.shape[0], len(unique_grupo_train)))\n",
        "for i, grupo_value in enumerate(unique_grupo_train):\n",
        "    if grupo_value in np.unique(X_val[:, grupo_index_val]):\n",
        "        grupo_dummies[:, i] = (X_val[:, grupo_index_val] == grupo_value).astype(int)\n",
        "\n",
        "# Create dummy features for all soil values\n",
        "soil_dummies = np.zeros((X_val.shape[0], len(unique_soil_train)))\n",
        "for i, soil_value in enumerate(unique_soil_train):\n",
        "    if soil_value in np.unique(X_val[:, soil_index_val]):\n",
        "        soil_dummies[:, i] = (X_val[:, soil_index_val] == soil_value).astype(int)\n",
        "\n",
        "# Define the indices of the features that need log transformation (river, cities, roads)\n",
        "log_indices_val = [\n",
        "    feature_files_val.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/validation/val_river_distance_raster.tif'),\n",
        "    feature_files_val.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/validation/val_cities_masked.tif'),\n",
        "    feature_files_val.index('/Users/guillermoromero/Documents/post-meds/data/policy-data/validation/val_road_distance_raster.tif')\n",
        "]\n",
        "\n",
        "# Create a column transformer for preprocessing\n",
        "preprocessor_val = make_column_transformer(\n",
        "    (OneHotEncoder(), [grupo_index_val, soil_index_val]),  # One-hot encode categorical features\n",
        "    (FunctionTransformer(np.log1p), log_indices_val),  # Apply log transformation\n",
        "    remainder='passthrough'  # Pass the remaining features as is\n",
        ")\n",
        "# Apply the preprocessing steps to the entire dataset\n",
        "X_preprocessed_val = preprocessor_val.fit_transform(X_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_soils_val = np.unique(X_val[:, grupo_index_val])\n",
        "print(unique_soils_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IH-I5xH0x42d"
      },
      "outputs": [],
      "source": [
        "y_pred_val = brfc.predict(X_preprocessed_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXYpYrfykVFZ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    joblib.dump(best_params, 'best_params.pkl')\n",
        "    joblib.dump(best_score, 'best_score.pkl')\n",
        "    joblib.dump(best_model, 'best_model.pkl')\n",
        "    joblib.dump(cv_results, 'cv_results.pkl')\n",
        "    joblib.dump(cv_results_df, 'cv_results_df.pkl')\n",
        "    joblib.dump(scorer, 'scorer.pkl')\n",
        "    joblib.dump(refit_time, 'refit_time.pkl')\n",
        "    joblib.dump(report, 'report.pkl')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bu_Y-p_rkYPq"
      },
      "outputs": [],
      "source": [
        "# Save the probability raster as a GeoTIFF file\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "output_file = os.path.join(output_folder, \"brfc-df-prediction-feature.tiff\")\n",
        "\n",
        "with rasterio.open(y_file) as src:\n",
        "    profile = src.profile\n",
        "    profile.update(dtype=rasterio.float32, count=1)\n",
        "\n",
        "prob_raster_reshaped = prob_raster.reshape((1, prob_raster.shape[0], prob_raster.shape[1]))\n",
        "\n",
        "with rasterio.open(output_file, 'w', **profile) as dst:\n",
        "    dst.write_band(1, prob_raster_reshaped[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iRMb_cgkb90"
      },
      "outputs": [],
      "source": [
        "# Report\n",
        "model_report = f'''\n",
        "\n",
        "Balanced Random Forest Classifier Model Report\n",
        "\n",
        "# Summary\n",
        "\n",
        "The Balanced Random Forest Classifier performed reasonably well on this task,\n",
        "with an accuracy of  {accuracy} and an F1-score of {f1}.\n",
        "However, there is room for improvement, particularly in the precision and recall for class 1.\n",
        "Future work could explore different models, additional feature engineering, or further hyperparameter tuning to improve performance.\n",
        "\n",
        "# Model Selection\n",
        "\n",
        "We chose to use a Balanced Random Forest Classifier for this task.\n",
        "This model is an ensemble method that combines the predictions of several base estimators\n",
        "built with a given learning algorithm in order to improve generalizability and robustness over a single estimator.\n",
        "It also handles imbalanced classes, which is a common problem in many machine learning tasks.\n",
        "\n",
        "Hyperparameter Tuning\n",
        "We used RandomizedSearchCV for hyperparameter tuning.\n",
        "This method performs a random search on hyperparameters, which is more efficient than an exhaustive search like GridSearchCV.\n",
        "\n",
        "The hyperparameters we tuned were:\n",
        "\n",
        "'n_estimators': The number of trees in the forest.\n",
        "'max_depth': The maximum depth of the tree.\n",
        "'min_samples_split': The minimum number of samples required to split a node.\n",
        "'min_samples_leaf': The minimum number of samples required at a leaf node.\n",
        "'bootstrap': Whether bootstrap samples are used when building trees.\n",
        "\n",
        "{param_grid}\n",
        "\n",
        "# Model Performance\n",
        "The best parameters found by RandomizedSearchCV were:\n",
        "\n",
        "Best parameters:, {best_params}\n",
        "\n",
        "\n",
        "\n",
        "With these parameters, the model achieved the following performance metrics:\n",
        "Best cross-validation score: {best_score}\n",
        "Best model:, {best_estimator}\n",
        "Scorer function:, {scorer}\n",
        "Refit time (seconds): {refit_time}\n",
        "Accuracy:, {accuracy}\n",
        "F1-score: {f1}\n",
        "\n",
        "# Testing Data\n",
        "\n",
        "Classification report:\n",
        "\n",
        "{report}\n",
        "\n",
        "#  TRAINING DATA Classificatin Report-Confusion Matrix\n",
        "\n",
        "Training confusion matrix:\n",
        "\n",
        "{train_cm}\n",
        "\n",
        "Training classification report:\n",
        "\n",
        "{train_cr}\n",
        "\n",
        "\n",
        "This indicates that the model correctly classified [1,1] instances of class 0\n",
        "and [2,2] instances of class 1,\n",
        "\n",
        "while misclassifying [1,2] instances of class 0 and [2,1] instances of class 1.\n",
        "\n",
        "CV Results:\n",
        "{cv_results_df}\n",
        "\n",
        "'''\n",
        "# Write the report to a Quarto markdown file\n",
        "with open('model_report.qmd', 'w') as f:\n",
        "    f.write(model_report)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
