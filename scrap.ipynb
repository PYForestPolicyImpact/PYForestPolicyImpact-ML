{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from shapely.geometry import shape\n",
    "from shapely.validation import explain_validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "current_dir = os.path.abspath('')\n",
    "\n",
    "# Search for the 'constants.py' file starting from the current directory and moving up the hierarchy\n",
    "project_root = current_dir\n",
    "while not os.path.isfile(os.path.join(project_root, 'constants.py')):\n",
    "    project_root = os.path.dirname(project_root)\n",
    "\n",
    "# Add the project root to the Python path\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import LUP_LABELED, DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lup_labeled = gpd.read_file(LUP_LABELED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Projected CRS: EPSG:32721>\n",
       "Name: WGS 84 / UTM zone 21S\n",
       "Axis Info [cartesian]:\n",
       "- E[east]: Easting (metre)\n",
       "- N[north]: Northing (metre)\n",
       "Area of Use:\n",
       "- name: Between 60°W and 54°W, southern hemisphere between 80°S and equator, onshore and offshore. Argentina. Bolivia. Brazil. Falkland Islands (Malvinas). Paraguay. Uruguay.\n",
       "- bounds: (-60.0, -80.0, -54.0, 0.0)\n",
       "Coordinate Operation:\n",
       "- name: UTM zone 21S\n",
       "- method: Transverse Mercator\n",
       "Datum: World Geodetic System 1984 ensemble\n",
       "- Ellipsoid: WGS 84\n",
       "- Prime Meridian: Greenwich"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lup_labeled.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hansen = rasterio.open(HANSEN_LOSSYEAR_FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hansen.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dst_crs = 'EPSG:32721'\n",
    "# Calculate the transformation needed for the reprojection\n",
    "transform, width, height = calculate_default_transform(\n",
    "    hansen.crs, dst_crs, hansen.width, hansen.height, *hansen.bounds)\n",
    "kwargs = hansen.meta.copy()\n",
    "kwargs.update({\n",
    "    'crs': dst_crs,\n",
    "    'transform': transform,\n",
    "    'width': width,\n",
    "    'height': height\n",
    "})\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''output_path = os.path.join(DATA_PATH, 'processing','hansen_output', 'hansen_reprojected.tiff')\n",
    "# Write the new file with the updated metadata\n",
    "with rasterio.open(output_path, 'w', **kwargs) as dst:\n",
    "    # Assuming you want to reproject and write the first band\n",
    "    reproject(\n",
    "        source=rasterio.band(hansen, 1),\n",
    "        destination=rasterio.band(dst, 1),\n",
    "        src_transform=hansen.transform,\n",
    "        src_crs=hansen.crs,\n",
    "        dst_transform=transform,\n",
    "        dst_crs=dst_crs,\n",
    "        resampling=Resampling.nearest\n",
    "    )'''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70842, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lup_labeled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lup_labeled.geometry.is_empty.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9972, 5)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A result of qgis processing creating sliver polygons removed with negative and positive buffers. \n",
    "lup_labeled[lup_labeled['geometry'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60870, 5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lup_labeled[lup_labeled['geometry'].notnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lup_labeled = lup_labeled[lup_labeled['geometry'].notnull()]\n",
    "# Create a list of geometries from the 'lup_labeled' DataFrame\n",
    "geometries = lup_labeled['geometry'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check each geometry for validity and print any that are invalid\n",
    "for geom in geometries:\n",
    "    if not shape(geom).is_valid:\n",
    "        print(explain_validity(shape(geom)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(DATA_PATH, 'processing','hansen_output', 'hansen_masked.tiff')\n",
    "\n",
    "# Open the reprojected raster file\n",
    "with rasterio.open(HANSEN_REPROJECTED) as src:\n",
    "    # Read the metadata of the file\n",
    "    meta = src.meta.copy()\n",
    "    \n",
    "    # Set the 'nodata' value for the output file to a number that represents 'nodata'\n",
    "    # This could be a specific value that you choose to represent 'nodata'\n",
    "    nodata_value = 255\n",
    "    meta.update(nodata=nodata_value)\n",
    "    \n",
    "    # Create a list of geometries from the 'lup_labeled' DataFrame\n",
    "    geometries = lup_labeled['geometry'].values\n",
    "    \n",
    "    # Create a mask for the raster using the geometries\n",
    "    out_image, out_transform = mask(src, geometries, crop=False, nodata=nodata_value, invert=True)\n",
    "    \n",
    "    # Update the metadata with the new transform and dimensions\n",
    "    meta.update({\"driver\": \"GTiff\",\n",
    "                 \"height\": out_image.shape[1],\n",
    "                 \"width\": out_image.shape[2],\n",
    "                 \"transform\": out_transform})\n",
    "\n",
    "    # Write the masked raster to a new TIFF file\n",
    "    with rasterio.open(output_path, 'w', **meta) as dst:\n",
    "        dst.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty array to store the final accumulated deforestation data\n",
    "accumulated_deforestation = np.zeros_like(destination_array, dtype=np.uint8)\n",
    "\n",
    "# Loop through each polygon\n",
    "for index, polygon in lup_labeled.iterrows():\n",
    "    # Determine the ten-year interval for the polygon\n",
    "    start_year = polygon['anho_capa']\n",
    "    end_year = start_year + 10\n",
    "\n",
    "    # Mask the 'lossyear' raster with the polygon\n",
    "    masked_lossyear, out_transform = mask(hansen, [polygon['geometry']], crop=True, all_touched=True)\n",
    "\n",
    "    # Select pixels within the ten-year interval\n",
    "    interval_mask = (masked_lossyear >= start_year) & (masked_lossyear < end_year)\n",
    "\n",
    "    # Update the accumulated deforestation array with the selected pixels\n",
    "    accumulated_deforestation[interval_mask] = masked_lossyear[interval_mask]\n",
    "\n",
    "# 'accumulated_deforestation' now contains the deforestation data for the ten-year intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store the accumulated pixels for each polygon\n",
    "\n",
    "\n",
    "accumulated_pixels = {}\n",
    "\n",
    "# Loop through each file\n",
    "for deforestation_file in deforestation_files:\n",
    "    # Construct the full path to the deforestation file\n",
    "    deforestation_path = os.path.join(HANSEN_TEN_INTERVALS_DIR, deforestation_file)\n",
    "\n",
    "    # Extract the starting year from the file name\n",
    "    start_year = int(deforestation_file.split('_')[1].split('-')[0])\n",
    "    start_year = 2000 + start_year\n",
    "    print(start_year)\n",
    "\n",
    "        \n",
    "\n",
    "         \n",
    "        # Loop through each polygon that corresponds to the current interval\n",
    "        for index, polygon in lup_labeled[lup_labeled['anho_capa'] == start_year].iterrows():\n",
    "            # Mask the raster with the polygon\n",
    "            out_image, out_transform = mask(destination_array, [polygon['geometry']], crop=True, all_touched=True)\n",
    "            \n",
    "            # Check if the polygon has been processed before\n",
    "            if index not in accumulated_pixels:\n",
    "                accumulated_pixels[index] = out_image.copy()\n",
    "            else:\n",
    "                # Otherwise, add the current mask to the accumulated pixels\n",
    "                accumulated_pixels[index] += out_image\n",
    "\n",
    "# 'accumulated_pixels' now contains the accumulated deforestation pixels for each polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select properties for validation set based on distance constraint\n",
    "\n",
    "def select_validation_set(gdf, distance_threshold=5000, validation_fraction=0.1):\n",
    "    validation_set = []\n",
    "    remaining_set = gdf.copy()\n",
    "    \n",
    "    while len(validation_set) < validation_fraction * len(gdf):\n",
    "        # Randomly select a property\n",
    "        selected_property = remaining_set.sample(1)\n",
    "        \n",
    "        # Append to validation set\n",
    "        validation_set.append(selected_property)\n",
    "        \n",
    "        # Calculate distance between the boundary of the selected property and boundaries of remaining properties\n",
    "        distances = remaining_set.boundary.distance(selected_property.boundary.squeeze())\n",
    "        \n",
    "        # Remove properties within the distance threshold from the remaining set\n",
    "        remaining_set = remaining_set[distances > distance_threshold]\n",
    "        \n",
    "        # Break if no more properties can be added\n",
    "        if len(remaining_set) == 0:\n",
    "            break\n",
    "    \n",
    "    return gpd.GeoDataFrame(pd.concat(validation_set, ignore_index=True))\n",
    "\n",
    "# Get validation set\n",
    "validation_gdf = select_validation_set(dissolved_clean_years)\n",
    "\n",
    "# Get training set by excluding validation set\n",
    "train_gdf = dissolved_clean_years[~dissolved_clean_years.index.isin(validation_gdf.index)]\n",
    "\n",
    "print(f\"Training set size: {len(train_gdf)}\")\n",
    "print(f\"Validation set size: {len(validation_gdf)}\")\n",
    "\n",
    "# Save training set to a GeoPackage file\n",
    "train_gdf.to_file(\"training_set.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "# Save validation set to a GeoPackage file\n",
    "validation_gdf.to_file(\"validation_set.gpkg\", driver=\"GPKG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select properties for validation set based on distance constraint\n",
    "def select_validation_set(gdf, distance_threshold=5000, validation_fraction=0.1):\n",
    "    validation_set = []\n",
    "    remaining_set = gdf.copy()\n",
    "    \n",
    "    while len(validation_set) < validation_fraction * len(gdf):\n",
    "        # Randomly select a property\n",
    "        selected_property = remaining_set.sample(1)\n",
    "        \n",
    "        # Append to validation set\n",
    "        validation_set.append(selected_property)\n",
    "        \n",
    "        # Calculate distance between the centroid of the selected property and centroids of remaining properties\n",
    "        distances = remaining_set.centroid.distance(selected_property.centroid.squeeze())\n",
    "        \n",
    "        # Remove properties within the distance threshold from the remaining set\n",
    "        remaining_set = remaining_set[distances > distance_threshold]\n",
    "        \n",
    "        # Break if no more properties can be added\n",
    "        if len(remaining_set) == 0:\n",
    "            break\n",
    "    \n",
    "    return gpd.GeoDataFrame(pd.concat(validation_set, ignore_index=True))\n",
    "\n",
    "# Get validation set\n",
    "validation_gdf = select_validation_set(gdf)\n",
    "\n",
    "# Get training set by excluding validation set\n",
    "train_gdf = gdf[~gdf.index.isin(validation_gdf.index)]\n",
    "\n",
    "print(f\"Training set size: {len(train_gdf)}\")\n",
    "print(f\"Validation set size: {len(validation_gdf)}\")\n",
    "\n",
    "# Save training set to a GeoPackage file\n",
    "train_gdf.to_file(\"training_set.gpkg\", driver=\"GPKG\")\n",
    "\n",
    "# Save validation set to a GeoPackage file\n",
    "validation_gdf.to_file(\"validation_set.gpkg\", driver=\"GPKG\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "policy",
   "language": "python",
   "name": "policy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
